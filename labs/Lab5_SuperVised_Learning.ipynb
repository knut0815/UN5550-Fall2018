{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> # Supervised learning (Chapter 5)\n",
    "## 10/04/2018 \n",
    "\n",
    "In Tuesday's class you had been introduced to the concepts of supervised learning and unsupervised learning in ML. \n",
    "* Exposure to the Scikit-learn toolbox\n",
    "* Metrics to evaluate performance of ML algorithm\n",
    "* Training / Validation / Testing\n",
    "* Awareness of some ML algorithms and their mathematical foundations\n",
    "* KNN classification using Iris dataset \n",
    "\n",
    "Particularly, the module focused on a supervised machine learning - classification. The chap 5 in the textbook provides the  basics of Machine learning and example codes in python are expicitly demonstrated. \n",
    "\n",
    "\n",
    "In this lab session, we will look into\n",
    "* Correctness meansure in detail with hands-on calculation\n",
    "* SciKit learning methods and useful functions\n",
    "* Install the required libraries for Project\n",
    "* Scikit-plot module\n",
    "* Importing and loading the MNIST Data \n",
    "\n",
    "Please also download, the week-05 jupyter notebook file, there are explanations that might be necessary for this excercise. \n",
    "\n",
    "**Dataset used:** Iris, breast cancer and MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit learn: More information in following link:  http://scikit-learn.org/stable/index.html\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using scikit-learn package load the toy dataset\n",
    "\n",
    "load iris, breast cancer and wine standard datasets and assign them to iris, cancer and wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading std datasets\n",
    "from  import \n",
    "\n",
    "\n",
    "# assign them to iris, cancer and wine\n",
    "iris = \n",
    "cancer = \n",
    "wine = \n",
    "\n",
    "# print(iris.data[0:10, ])\n",
    "# print (iris.target)\n",
    "print(type(iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lab session, we will load some of the large datasets for regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1A. Read Section 5.3 and from class material week-05, what does the following represent\n",
    "    i. Columns\n",
    "    ii. Rows\n",
    "    iii. Target\n",
    "\n",
    "    a. How many rows and columns are there in the iris, cancer and wine.\n",
    "    b. what is the label vectors in this datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  From section 5.3 and week-05, the input data and its structure are described\n",
    "\"\"\"\n",
    " The n_samples and n_features are the size of an array. \n",
    "\n",
    " The columns represent: \n",
    " whereas the rows represent: \n",
    " Target represent:  \n",
    " \n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"No of samples and features in iris datasets is {} and {} respectively\".format(iris.data.shape[0], iris.data.shape[1]))\n",
    "print (\"No of samples and features in cancer datasets is {} and {} respectively\".format(cancer.data.shape[0], cancer.data.shape[1]))\n",
    "print (\"No of samples and features in wine datasets is {} and {} respectively\".format(wine.data.shape[0], wine.data.shape[1]))\n",
    "\n",
    "\n",
    "print (\"iris:\\n\", iris.target_names)\n",
    "print (\"cancer:\\n\", cancer.target_names)\n",
    "print (\"wine:\\n\", wine.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B. From the link below: learn about the dataset loading utilities. How do you access these dataset. \n",
    "\n",
    "http://scikit-learn.org/stable/datasets/index.html\n",
    "\n",
    "### iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We can access the scikit datasets by using .data, **.target and **.DESCR \"\"\"\n",
    "print(iris.data [0:5,])\n",
    "print(iris.target)\n",
    "print (iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for cancer dataset. \n",
    "\"\"\" We can access the scikit datasets by using .data, **.target and **.DESCR \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We can access the scikit datasets by using .data, **.target and **.DESCR \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import other important packages necessary for supervised learning. The below links gives information and significance of each sklearn modules.\n",
    "\n",
    "https://docs.python.org/3/library/math.html\n",
    "\n",
    "https://docs.python.org/2/library/random.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/neighbors.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot for plotting framework\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "# for ploting in within notebook\n",
    "% matplotlib inline \n",
    "\n",
    "# import numpy\n",
    "import numpy as np\n",
    "\n",
    "# import counter from collections.\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# import math and random modules\n",
    "import math, random\n",
    "\n",
    "\n",
    "# sklearn is most important module for supervised learning. For knn import neighbors, metrics\n",
    "# the need for sklearn's above modules are detailed in the link provided above. \n",
    "\n",
    "# import neighbors and metrics\n",
    "from  import \n",
    "\n",
    "# look into metrics and import confusion matrix function\n",
    "\n",
    "\n",
    "# import train and test split function from sklearn\n",
    "\n",
    "\n",
    "# import cross validation score from sklearn validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We know that Sci-kit learn input data is structured in Numpy arrays, organized as\n",
    "\n",
    "### X := n_samples x  n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Assign the data of iris to Xi and target class to yi (beware of case sensitive variables defined here)\n",
    "Similarly, assign Xc and yc to cancer data and target respectively\n",
    "\n",
    "\"\"\"\n",
    "#i suffix for iris dataset\n",
    "Xi = \n",
    "yi = \n",
    "\n",
    "\n",
    "# c suffix for cancer dataset\n",
    "Xc =\n",
    "yc =\n",
    "\n",
    "\n",
    "# w suffix for wine dataset\n",
    "Xw = \n",
    "yw = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 KNN \n",
    "\n",
    "To implement KNN, following steps are to be followed:\n",
    "1. Pick a value of $k$: the number of neighbors we wish to use for classification.\n",
    "2. Compute some measure between the test sample and each training sample. \n",
    "3. Sort the computed distances in increasing order based on distance values\n",
    "4. for the $k$ closest training samples, return the most frequent class as the predicted class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 A. Scikit learn model selection \n",
    "\n",
    "\n",
    "### In section 5.3, page 75, the dataset is randomly split to train and test data. The code uses random function with permutation and a ravel() function. Let us use scikit learn function to perform the similar action.\n",
    "\n",
    "### scikit module, under model_selection function, import the function to split the dataset into test and train. \n",
    "\n",
    "    * Test size (iris): 33% \n",
    "    * Test size (cancer and wine): 1/4th of sample\n",
    "    * random state : 5\n",
    "\n",
    "    Refer link below to learn more:\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "    Lets suffix i--> for iris dataset, c--> Cancer and w--> wine datasets  \n",
    "    \n",
    "#### Hopefully the module from train_test_split is imported from cross validation library. If not import below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iris dataset\n",
    "Xitrain, Xitest, yitrain, yitest = train_test_split()\n",
    "\n",
    "# for cancer dataset\n",
    "Xctrain, Xctest, yctrain, yctest = \n",
    "\n",
    "# for wine dataset\n",
    "Xwtrain, Xwtest, ywtrain, ywtest = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Observe the size of test and train variables **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Iris Dataset\")\n",
    "print(\"data shape (X):\", Xi.shape, \"and\",  \"Target shape (y):\", yi.shape)\n",
    "print(\"Training shape:\", Xitrain.shape, \"and\",  \"Training targets shape:\", yitrain.shape)\n",
    "print(\"Testing shape:\", Xitest.shape, \"and\",  \"Testing targets shape:\", yitest.shape)\n",
    "\n",
    "print (\"\\n\\n\")\n",
    "\n",
    "print(\"For Cancer Dataset\")\n",
    "print(\"data shape (X):\", Xc.shape, \"and\",  \"Target shape (y):\", yc.shape)\n",
    "print(\"Training shape:\", Xctrain.shape, \"and\",  \"Training targets shape:\", yctrain.shape)\n",
    "print(\"Testing shape:\", Xctest.shape, \"and\",  \"Testing targets shape:\", yctest.shape)\n",
    "\n",
    "\n",
    "print (\"\\n\\n\")\n",
    "print(\"For wine Dataset\")\n",
    "print(\"data shape (X):\", Xw.shape, \"and\",  \"Target shape (y):\", yw.shape)\n",
    "print(\"Training shape:\", Xwtrain.shape, \"and\",  \"Training targets shape:\", ywtrain.shape)\n",
    "print(\"Testing shape:\", Xwtest.shape, \"and\",  \"Testing targets shape:\", ywtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B. Training the model\n",
    "\n",
    "### for Knn classification, use 5 neighbors initially "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" For iris dataset analysis \"\"\"\n",
    "X_train = Xitrain\n",
    "y_train = yitrain\n",
    "X_test =  Xitest\n",
    "y_test =  yitest\n",
    "target_names = iris.target_names\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" For cancer dataset analysis \"\"\"\n",
    "# X_train = Xctrain\n",
    "# y_train = yctrain\n",
    "# X_test =  Xctest\n",
    "# y_test =  yctest\n",
    "# target_names = cancer.target_names\n",
    "\n",
    "\n",
    "\"\"\" For wine dataset analysis \"\"\"\n",
    "# X_train = Xwtrain\n",
    "# y_train = ywtrain\n",
    "# X_test =  Xwtest\n",
    "# y_test =  ywtest\n",
    "# target_names = wine.target_names\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = ?? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the classifier using the training data, use fit()\n",
    "knn.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the prediction of the test set using predict()\n",
    "predict = knn.predict()\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "accuracy_score( , )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We saw that the accuracy score for iris dataset was above 0.95. For the same neighbor, how is the accuracy score for different datasets.\n",
    "\n",
    "* what was the accuracy score for breast cancer datasets?\n",
    "* What was the accuracy score for wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note down the observation of accuracy class here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3C. Performance Measures\n",
    "\n",
    "In the class, you had looked into performance measures for the model classification. TP, FP, TN, FN and other metrics of interest. let us again use the code from class for confusion matrix calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix code from class week 05\n",
    "C = np.empty([3,3])\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        C[i,j] = np.sum(np.logical_and(predict==i, y_test==j))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we know how the confusion matrix in form of code. We could see, if there are any in-built functions to do the same ?\n",
    "\n",
    "### Import confusion matrix from sklearn metrics and solve the same problem set. Do you observe any difference ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion matrix from sklearn metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix( , ) \n",
    "# Note that both the confusion matrix solutions match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Metrics\n",
    "\n",
    "### From page no 73, the performance metrics are defined. Create a functions for accuracy, sensitivity or recall, specificity, precision and negative predictive values. \n",
    "\n",
    "Refer to either class notes or textbook for these definitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tp, fp, fn, tn):\n",
    "    \n",
    "    \n",
    "    return (\"insert your code here\")\n",
    "\n",
    "\n",
    "def recall():\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def specificity ():\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def precision():\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def NegativePredictiveValue():\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def f1_score(tp, fp, fn, tn):\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.A. Test the function above for the binary case as detailed in Page [74] What values does the function return for accuracy, recall, specificity, precision, NegativePredictiveValue and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=3370.\n",
    "tn=73.\n",
    "fn=7.\n",
    "fp=690.\n",
    "\n",
    "# Test the previous written code here.\n",
    "\n",
    "print(\"Accuracy: \", accuracy(tp,fn,fp,tn))\n",
    "print(\"recall: \", recall(tp,fp,fn,tn))\n",
    "print(\"specificity: \", specificity(tp,fp,fn,tn))\n",
    "print(\"precision: \", precision(tp,fp,fn,tn))\n",
    "print(\"NegativePredictiveValue: \", NegativePredictiveValue(tp,fp,fn,tn))\n",
    "print(\"f1_score:\", f1_score(tp,fp,fn,tn))\n",
    "print (tp, tn, fn, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.B. Work on the following and using the function developed earlier, compute the performance metrices. \n",
    "We have 100 objects: 90 of the positive class, and 10 of the negative class.\n",
    "* The supervised learning algorithm correctly classified 45 of the positive class, and 5 of the negative class.\n",
    "* The supervised learning algorithm correctly classified 10 of the positive class, and 9 of the negative class.\n",
    "\n",
    "We have 100 objects: 50 of the positive class, and 50 of the negative class.\n",
    "* The supervised learning algorithm correctly classified 40 of the positive class, and 10 of the negative class.\n",
    "* The supervised learning algorithm correctly classified 40 of the positive class, and 40 of the negative class.\n",
    "\n",
    "\n",
    "What is the computed accuracy, precision, recall and f1 score for the above problem?\n",
    "(Question adapted from UN5550-assignment 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that we have 100 objects: 90 of the positive class, and 10 of the negative class.\n",
    "#a) Correctly classified 45 of the positive class, and 5 of the negative class.\n",
    "tp=\n",
    "tn=\n",
    "fn=\n",
    "fp=\n",
    "print(\"\\n Output A \\n\")\n",
    "print(accuracy(tp,fn,fp,tn))\n",
    "print(precision(tp,fp,fn,tn))\n",
    "print(recall(tp,fp,fn,tn))\n",
    "print(f1_score(tp,fp,fn,tn))\n",
    "\n",
    "\n",
    "# Correctly classified 10 of the positive class, and 9 of the negative class.\n",
    "tp=\n",
    "tn=\n",
    "fn=\n",
    "fp=\n",
    "print(\"\\n Output B \\n\")\n",
    "print(accuracy(tp,fn,fp,tn))\n",
    "print(precision(tp,fp,fn,tn))\n",
    "print(recall(tp,fp,fn,tn))\n",
    "print(f1_score(tp,fp,fn,tn))\n",
    "\n",
    "\n",
    "# We know that we have 100 objects: 50 of the positive class, and 50 of the negative class.\n",
    "# correctly classified 40 of the positive class, and 10 of the negative class.\n",
    "tp=\n",
    "tn=\n",
    "fn=\n",
    "fp=\n",
    "print(\"\\n Output C \\n\")\n",
    "print(accuracy(tp,fn,fp,tn))\n",
    "print(precision(tp,fp,fn,tn))\n",
    "print(recall(tp,fp,fn,tn))\n",
    "print(f1_score(tp,fp,fn,tn))\n",
    "\n",
    "# Correctly classifies 40 of the positive class, and 40 of the negative class.\n",
    "#  correctly classified 40 of the positive class, and 40 of the negative class\n",
    "tp=\n",
    "tn=\n",
    "fn=\n",
    "fp=\n",
    "print(\"\\n Output D \\n\")\n",
    "print(accuracy(tp,fn,fp,tn))\n",
    "print(precision(tp,fp,fn,tn))\n",
    "print(recall(tp,fp,fn,tn))\n",
    "print(f1_score(tp,fp,fn,tn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4C. Now that we have looked into performance metric, we can use the inbuilt function to measure the score or values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets try to use classification report from metrics module. ALso individual performance metrics we coded are \n",
    "also available as functions in sklearn metrics\n",
    "\n",
    "\"\"\"\n",
    "# import classification report and F1 score\n",
    "from \n",
    "\n",
    "print(classification_report())\n",
    "print (\"F1 score: \", f1_score( , ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4D. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import cross_val_score from model selection library of sklearn. Learn more from the link given below:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#examples-using-sklearn-model-selection-cross-val-score\n",
    "\n",
    "### Also in the below code, using for-loop check what is the impact of changing the no of neighbors in Knn for two different weights in the KNN. Measure the performance using cross_val_score for test samples and scoring for accuracy. \n",
    "\n",
    "#### Note: The no. of neightbors should be less than the test samples. Take either 20 or 25 or 30 \n",
    "\n",
    "Hint: Take two for-loops, first loop should take care of weights and second loop needs to take care of the no of neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required library for cross_val_score (see link above)\n",
    "\n",
    "# Create for loop for weights first\n",
    "for weights in ['uniform', 'distance']:\n",
    "    k_range = range(1, )\n",
    "    k_score= []\n",
    "    # create for loop for neighbors\n",
    "    for k in k_range:\n",
    "        # knn code here\n",
    "        knn = # finish this code\n",
    "        \n",
    "        # computing the score.\n",
    "        scores = cross_val_score(, , , cv = 5, scoring ='accuracy') # finish this code\n",
    "        \n",
    "        # append the score to k_score\n",
    "        k_score.append(scores.mean())\n",
    "\n",
    "        \n",
    "    # plot inside first loop to plot for each weights    \n",
    "    plt.figure()\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Use line plot and observe\n",
    "    plt.plot(k_range, k_score)\n",
    "    plt.xlabel(' Value of K for knn')\n",
    "    plt.ylabel('Cross-Validated Accuracy')\n",
    "    plt.suptitle('%s' %(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the current dataset and plots, which weight seems to more feasible??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SVM\n",
    "\n",
    "## 5A. From Section 5.7.2, SVM learning technique is illustrated. Now use the sklearn function to analyze the dataset using SVM.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" For wine dataset analysis \"\"\"\n",
    "# We will use the previous used partition of data for train and test datasets.\n",
    "X_train = Xwtrain\n",
    "y_train = ywtrain\n",
    "X_test =  Xwtest\n",
    "y_test =  ywtest\n",
    "target_names = wine.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use Kernel SVM model to classify.\n",
    "# Fitting to the Training set\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC( , random_state = 0)\n",
    "\n",
    "\n",
    "# similar to knn fit, train the mdoel.\n",
    " \n",
    "\n",
    "#Predicting the Test Set\n",
    "y_pred =  \n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# You will learn more about the scikit plot in the below sections especially for confusion matrix to plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5B. Explain, why is KNN called the lazy learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOL: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5C. What do you think are the main challenges of Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOL:\n",
    ".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Scikit-plot  \n",
    "\n",
    "Go to link below. Make note of useful functions that could be applied for the current problem and for project.\n",
    "\n",
    "## https://scikit-plot.readthedocs.io/en/stable/Quickstart.html\n",
    "\n",
    "scikit-plot uses both scikit learn and matplot lib to generate the plots useful for ML problems. One such plot is to plot confusion matrix. \n",
    "\n",
    "Dont forget to install scikit plot, use **pip install scikit-plot**\n",
    "\n",
    "Use functions from scikit plot for plotting confusion matrix and learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6A. Learning curves provided by scikit plot for any classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" For wine dataset analysis \"\"\"\n",
    "X_train = Xwtrain\n",
    "y_train = ywtrain\n",
    "X_test =  Xwtest\n",
    "y_test =  ywtest\n",
    "target_names = wine.target_names\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "\n",
    "# Import the scikit plot and use plot_learning_curve to visualize the learning curves of KNN\n",
    "import scikitplot as skplt\n",
    "\n",
    "# Look for estimator modules in the link provided above. Check the performance of KNN\n",
    "skplt.            ( , , )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B. Lets try to visualize the confusion matrix using functions from scikitplot. From the link provided visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" For iris dataset analysis \"\"\"\n",
    "X_train = Xitrain\n",
    "y_train = yitrain\n",
    "X_test =  Xitest\n",
    "y_test =  yitest\n",
    "target_names = iris.target_names\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 5)\n",
    "# trained the classifier using the training data, use fit()\n",
    "knn.fit(X_train, y_train)\n",
    "# computed the prediction of the test set using predict()\n",
    "predict = knn.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn import confusion matrix if it wasnt.\n",
    "# either use confucion matrix to infer \n",
    "print(confusion_matrix(predict, y_test))\n",
    "\n",
    "\n",
    "\n",
    "# The example code is available in the link provided above.\n",
    "# import scikit plot without fail to use this function\n",
    "import \n",
    "# or directly use the plot_confusion matrix to plot\n",
    "plot = \n",
    "\n",
    "# Observe if the both computed confusion matrix matches with the visualized confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read through the below link and apply the estimator module of plot_feature_importance to the problem defined in class. (iris dataset and for Knn)\n",
    "https://scikit-plot.readthedocs.io/en/stable/estimators.html\n",
    "\n",
    "    scikitplot.estimators.plot_feature_importances(clf, title='Feature Importance', feature_names=None, max_num_features=20, order='descending', x_tick_rotation=0, ax=None, figsize=None, title_fontsize='large', text_fontsize='medium')\n",
    "    \n",
    "### What does this estimator convey ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: \n",
    "\n",
    "        There would be a need to install and work with the MNIST data and associated python package. Go through the requiremnents of the Project 4. \n",
    "\n",
    "    * In this session we would play around with data\n",
    "    * First download the files from the link provided\n",
    "    * Follow instruction to unzip four files\n",
    "    * To use mnist package, we need to install one. Details are available in link : \n",
    "https://pypi.org/project/python-mnist/\n",
    "\n",
    "Dataset link: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Project 4:  UN5550-Fall2018/projects/Project04.ipynb \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# successfully installed mnist module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --user python-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST_Dataset_Loader doesnt work, instead \"from mnist import MNIST\" works. As given in sample code.\n",
    "\n",
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors, metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets available in the link are to be uploaded to the jupyter notebook. Similar to the csv or excel files, the required files neeeds to be uploaded into the folder. \n",
    "\n",
    "* Using the terminal console unzip *.gz file (best if performed in terminal than in jupyter notebook)\n",
    "\n",
    "* Upon successful unzipping of compressed data, four files would be present without the extension *.gz.\n",
    "\n",
    "Follow the instruction detailed in the Project description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following the instruction from project, import MNIST\n",
    "\n",
    "# This loads the dataset from system\n",
    "data = \n",
    "\n",
    "train_images, train_labels = \n",
    "test_images, test_labels =  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You could load the features and samples into the variables Xtrain, ytrain & Xtest, ytest.\n",
    "\n",
    "### Lets upload only few of those samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont forget to transform into array \n",
    "Xtrain = np.array() \n",
    "ytrain = \n",
    "\n",
    "Xtest = \n",
    "ytest = \n",
    "\n",
    "print (\"Xtrain shape:\", Xtrain.shape, \"ytrain shape: \", ytrain.shape)\n",
    "print (\"Xtest shape:\", Xtest.shape, \"ytest shape: \", ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit() and Predict () is similar to what you have learnt in class and in today's lab session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scikit-plot to get the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for imshow(). Planned not to reveal this, instead wait for them to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use imshow() to plot the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful link\n",
    "\n",
    "    The link below provides a good example and practice for Scikit-learn. Please look into subcategories such as i. Classification ii. Dataset examples and iii. Decomposition and many more \n",
    "Link: http://scikit-learn.org/stable/auto_examples/index.html#general-examples\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the Project 4 is already uploaded in the course repository. There is a necessity to download 4 files from the link provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================== END ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 05 - Jupyter Notebook (For reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets # import standard datasets\n",
    "\n",
    "iris = datasets.load_iris() # load iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(iris))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data[:6, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris.data.shape[1]\n",
    "samples = iris.data.shape[0]\n",
    "print (\"number of samples = %g, number of features = %g\"%(samples,features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "# create an instance of K-nearest neighbor classifer\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "# train the classifier using the all the samples, except the last one\n",
    "knn.fit(iris.data[:-1], iris.target[:-1] )\n",
    "\n",
    "# compute the prediction of the last sample according to the model\n",
    "predict  = knn.predict(iris.data[-1:])\n",
    "\n",
    "print (\"actual classification: \" + iris.target_names[iris.target[-1]])\n",
    "print (\"predicted classification: \" + iris.target_names[predict[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = iris.data[:,:2] # take only the first two columns\n",
    "y = iris.target\n",
    "\n",
    "# train classifier\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 41)\n",
    "knn.fit(x, y)\n",
    "\n",
    "# setup mesh\n",
    "h = 0.1\n",
    "x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "xg, yg = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# run classifier on mesh\n",
    "z = knn.predict(np.c_[xg.ravel(), yg.ravel()])\n",
    "\n",
    "# specifying some colors\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# plot classifier\n",
    "zg = z.reshape(xg.shape)\n",
    "_ = plt.figure()\n",
    "_ = plt.pcolormesh(xg,yg,zg,cmap=cmap_light)\n",
    "\n",
    "# overlay with training data points\n",
    "_ = plt.scatter(x[:,0],x[:,1],c=y, cmap=cmap_bold, edgecolor='k')\n",
    "\n",
    "# setting the aspect ratio so we can visualize distance properly\n",
    "_ = plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 11)\n",
    "\n",
    "# train the classifier using the all the samples\n",
    "knn.fit(iris.data, iris.target)\n",
    "\n",
    "# compute the prediction each sample using the model\n",
    "predict  = knn.predict(iris.data)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predict,iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.empty([3,3])\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        C[i,j] = np.sum(np.logical_and(predict==i, iris.target==j))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.33, random_state=10)\n",
    "# note: test_size is a number between 0 and 1,\n",
    "# random_state is the seed used by the random number generator.  Here, I've fixed it to 10 for reproducibility\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "# train the classifier using the training data\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# compute the prediction of the test set using the model\n",
    "predict  = knn.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (\"accuracy = %g \"%(accuracy_score(predict,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.empty([3,3])\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        C[i,j] = np.sum(np.logical_and(predict==i, y_test==j))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 11)\n",
    "\n",
    "accuracy = np.zeros((10,))\n",
    "\n",
    "for i in range(10):\n",
    "    # split the data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4)\n",
    "\n",
    "    # train the classifier using the training data\n",
    "    knn.fit(x_train, y_train)\n",
    "\n",
    "    # compute the prediction of the test set using the model\n",
    "    predict  = knn.predict(x_test)\n",
    "\n",
    "    accuracy[i] = accuracy_score(predict,y_test)\n",
    "\n",
    "print (\"mean accuracy = %g \"%(accuracy.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Accuracy Plot')\n",
    "_ = ax1.boxplot(accuracy) \n",
    "xderiv = np.ones(accuracy[:].shape)+(np.random.rand(10,)-0.5)*0.1\n",
    "_ = plt.plot(xderiv,accuracy[:] ,'ro',alpha=0.3) # add a jigger plot of data points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
