{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> ## Supervised Learning - Part II (Chapter 5)\n",
    "\n",
    "From Tuesday's class, you had an opportunity learn few more classification methods and modules to validate.\n",
    "\n",
    "    Cross-Validation\n",
    "    Learning Curves\n",
    "    Support Vector Machines\n",
    "    Random Forest\n",
    "    \n",
    "* In previous lab session, we looked into KNN, preformance metrics, confusion matrix and scikit plot.\n",
    "\n",
    "* In this lab session, we will look into the following\n",
    "\n",
    "    * We can look into importing and loading the MNIST Data and install the required libraries (If someone is still facing the trouble loading)\n",
    "    * SciKit learning methods and useful functions\n",
    "    * SVM's different Kernels, features and ways to tune the model\n",
    "    * Similarly Random forest and its model tuning \n",
    "\n",
    "\n",
    "Please also download, the week-05 jupyter notebook file, there are explanations that might be necessary to get insightful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Links: \n",
    "#### http://scikit-learn.org/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\" Decision boundary plotting function from Hands-On Machine Learning with Scikit-Learn\n",
    "# and TensorFlow \"\"\"\n",
    "\n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "    \n",
    "    \n",
    "# Function adapted from source: https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n",
    "\n",
    "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # fit the estimator\n",
    "    model.fit(X, y)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap, clim=(y.min(), y.max()),\n",
    "                           zorder=1)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the following steps:\n",
    "* Import library numpy, matplotlib, datasets, neighbors, metrics, model selection libraries\n",
    "* Load cancer std dataset using sklearn datasets and assign to \"cancer\"\n",
    "* Look into the various attributes cancer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the link, additional functions and its attributes are illustrated. \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "### Lets have a very quick recap of KNN and model fitting\n",
    "Perform the below actions:\n",
    "* From the sklearn model selection module import train test split, use features like random_state, stratify \n",
    "* Use KNN method with 3 neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\" No of neighbors can be fixed as shown below, but is not efficient. we do not know the model \n",
    "characteristic beforehand and hence to evaluate  \"\"\"\n",
    "clf = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute and visualize the training and testing accuracies when number of neighbors in KNN is changed from 1 to 30\n",
    "\n",
    "Use the following parameters \n",
    "* Test size  = 0.3 \n",
    "* To maintain consistency among students, let use random state and of 55, 60, 65 and 70\n",
    "* use a for loop to test knn classifiers from 1 to 30 neighbors, inside looping function you will have to build the knn, train the model and compute the score i.e., knn.score (Xtest, ytest)\n",
    "* The training and testing accuracy for each neighbor (iteration) can be stored and plotted after looping\n",
    "* At what choice do you observe large variance and which random state do ML converge\n",
    "* What is the difference between this plot vs the one discussed in learning curves in class?\n",
    "* look into time module in python, you can make a note of time to train the model for each iteration, this could be used in the below sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" use plot to visualize the duration vs n_neighbors \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machines\n",
    "The following reference from Tuesday class gives be very good overview of SVM classifiers and other advanced methods.\n",
    "Refer to page 16 of the presentation link. SVM classification with kernel trick is visualized. \n",
    "https://med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf\n",
    "\n",
    "\n",
    "Some of the different kernels that can be accessed by Scikit learn are shown below. \n",
    "* Linear \n",
    "* Gaussian \n",
    "* Exponential\n",
    "* Polynomial \n",
    "* hybrid\n",
    "* Sigmoidal\n",
    "\n",
    "Read the below reference for accessing kernels in scikit learn\n",
    "http://scikit-learn.org/stable/modules/svm.html#svm-kernels\n",
    "\n",
    "We will use Linear, Gaussian and polynominal for current dataset problem and possibly we could try different kernel approach during regression lab session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "        - The cancer dataset needs to be loaded for working on the below problem\n",
    "        - Import necessary svm modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. For simiplicity lets consider only the first two features for classification i.e., mean radius and mean texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take mean radius and mean texture for cancer classification data, assign to \"X\" and cancer target names to \"y\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Using link provided above for reference, build SVM with different kernel type and use default c, gamma, degree, coeff0 necessary\n",
    "    - Note: each svm kernel type attribute differs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.) Apply \"linear Kernel\" and observe what happens with different values of gamma and C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using SVC fit complete data of cancer data and target features \n",
    "svc = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Once, we have fit the linear SVC kernel, use function plot_predictions function to visualize the classifiers and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Use the function provided in the early cell plot_prediction to visualize \"\"\"\n",
    "plot_predictions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe what happens when you fine tune the values of C and gamma ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Similar to linear kernel, train the svc for rbf kernel and visualize. Observe the pattern for different values of c and gamma functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.) Apply \"rbf Kernel\" and observe what happens with different values of gamma and C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Once, we have fit the rbf SVC kernel, use function plot_predictions function to visualize the SVC working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_predictions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Similar to linear kernel, train the svc for poly kernel and visualize. \n",
    "    - Do you still need gamma? \n",
    "    - Check what all attributes are useful for this kernel, please look into link or sckikit learn help\n",
    "    - Change for different degrees starting from 1 to 5\n",
    "    - There might be a case, where jupyter notebook takes almost impossible time to train. In this case, you may need to interrupt the Jupyter Kernel and restart "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.) Apply \"polynomial Kernel\" and observe what happens with different values of degree, C & Coeff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try for degree  = 1, 3, 5 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_predictions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "1. What do you observe as you increase the degree of polynomial (with respect to the plot and time for computation)\n",
    "2. See what happens when d=1 and linear kernel problem\n",
    "3. In this case how do you know that degree of polynomial you have reached is optimal ?\n",
    "4. What do you think of Pros and Cons of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Solutions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Now that you have been introduced to fine tunning of parameters, apply SVC to complete dataset and observe the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree code from class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(max_depth=2)\n",
    "x_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3, random_state = 5)\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "                         feature_names=cancer.feature_names,  \n",
    "                         class_names=cancer.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "## 1. Develop RF classifier and fine tune the parameter to optimize the result.\n",
    "\n",
    "- Import the necessary modules for random forest classifiers\n",
    "- Use random state = 5\n",
    "- test size = 40%\n",
    "- check the score\n",
    "- check the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion about the visualizing the tree from random forest\n",
    "\n",
    "https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can modify the code for Decision Tree visualization to Random forest\n",
    "from sklearn.tree import export_graphviz\n",
    "RFmodel = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3, random_state = 3)\n",
    "RFmodel.fit(X_train, y_train)\n",
    "\n",
    "import graphviz \n",
    "\n",
    "visual_tree = RFmodel.estimators_[4]\n",
    "export_graphviz(visual_tree, out_file = 'best_tree.dot', feature_names = cancer.feature_names,\n",
    "                precision = 2, filled = True, rounded = True, max_depth = None)\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine tunning parameters for RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Lets visualize the RF based classifiers and dataset as shown in the previous section about SVM\n",
    "    - Make sure, you are training the model with only two features (it is upto you).\n",
    "    - Based on important features, I found column 3 and 8 are cruicial for proper classification\n",
    "    - Using function visualize_classifier(), visualize the RF and scatter plots of data\n",
    "    \n",
    "    ex:\n",
    "    visualize_classifier(classifier, iris.data[:, :2], iris.target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# develop the random forest model with 50 estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# develop the random forest model with 500 estimators\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you observe any overfitting or underfitting for any change in parameter change? Make a note of, if yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
