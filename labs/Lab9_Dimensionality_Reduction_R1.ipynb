{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Dimensionality Reduction\"></a>\n",
    "\n",
    "# Lab 09 - Dimensionality Reduction\n",
    "\n",
    "***\n",
    "In this lab session we will learn\n",
    "   * Data Compression via PCA (Principle Component Analysis)\n",
    "   * PCA and others using scikit-learn function\n",
    "   * Imputer from sklearn preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Dimensionality reduction summarizes the information contents of a higher dimension dataset which is usually difficult to understand & visualize and transforming them into lower dimension space. This approach of dimensionality reduction is also referred to as feature extraction or Data Compression [1]. __\n",
    "\n",
    "[1]. Sebastian Raschka. 2015. Python Machine Learning. Packt Publishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA\n",
    "* Unsupervised linear transformation\n",
    "* Applications include dimensionality reduction, exploratory data analysis, de-noisinf signals etc\n",
    "* Finds the direction of max variuance in high dimension data and projects into new subspace\n",
    "* No of components can be equal to or less than the original \n",
    "\n",
    "In Tuesday's class, you had opportunity to look into applying PCA and LDA for iris and blob dataset respectively. Now, lets apply PCA & LDA for **wine dataset** using function from scikit learn. Before dwelling into inbuilt functions, it would be worth to look \n",
    "* into the functioning of PCA by computing covraiance and eigen values\n",
    "* procedure to select the no of components for higher dimension datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 1.1 Principal Component Analysis - Internal working of PCA (without scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical approach for PCA was detailed in the class. Refer to week09 lecture notes. \n",
    "\n",
    "Steps to implement PCA is briefed below:\n",
    "1. Standardize the dataset\n",
    "2. construct the covariance matrix\n",
    "3. get the eigen vectors and values by decomposing the covariance matrix\n",
    "4. opt for 'n' eigen vectors that correspons to 'n' largest eigen values where n is the no of components that is needed to be reduced to.\n",
    "5. Project mtrix from top 'n' eigen vectors\n",
    "6. transform dataset using project matrix to new 'n' dimenional feature subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Standardize the dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardize the dataset\n",
    "x = StandardScaler().fit_transform(df)\n",
    "\n",
    "# Use pandas dataframe for easy handling\n",
    "X = pd.DataFrame(x, columns = wine.feature_names )\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Covariance matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "CovMatrix = np.cov(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Eigen Vectors and Values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eigen_val, eigen_vec = np.linalg.eig(CovMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Opt for 'n' eigen vectors that correspons to 'n' largest eigen values__\n",
    "* __Project matrix from top 'n' eigen vectors__\n",
    "* __Transform dataset using project matrix to new 'n' dimenional feature subspace__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constructing a projection matrix\n",
    "eigen_pairs = [(np.abs(eigen_val[i]), eigen_vec[:,i]) for i in range(len(eigen_val))]\n",
    "# plt.bar(eigen_pairs)\n",
    "eigen_pairs.sort(reverse = True)\n",
    "# print (eigen_pairs\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))\n",
    "# print (w)\n",
    "\n",
    "PCA_X = pd.DataFrame(X.dot(w))\n",
    "PCA_X.columns = ['PC 1', 'PC 2']\n",
    "species = pd.DataFrame(wine.target, columns =['class'])\n",
    "PCA_X = pd.concat([PCA_X, species], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Visualizing the PC1 and PC2 gives us few insight__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 Component PCA', fontsize = 20)\n",
    "\n",
    "\n",
    "targets = [0 , 1, 2]\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = PCA_X['class'] == target\n",
    "    ax.scatter(PCA_X.loc[indicesToKeep, 'PC 1'], PCA_X.loc[indicesToKeep, 'PC 2'], c=color, s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 1.2 How many dimension should we keep ??\n",
    "\n",
    "Choosing the right no of dimensions- \n",
    "\n",
    "* Cumulative percentage of variance - 70% to 95%\n",
    "* Scree plot - plot of eigen values, look for elbow\n",
    "* The Broken Stick \n",
    "* Size of variance - variance greater than 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (eigen_val)\n",
    "\n",
    "tot = sum(eigen_val)\n",
    "var_exp = [(i/tot) for i in sorted(eigen_val, reverse = True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "plt.plot(range(1,14), var_exp,'-*')\n",
    "plt.step(range(1,14), cum_var_exp, c='r', where='mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cum_var_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn method for PCA\n",
    "\n",
    "Now using scikit learn modules, implement the PCA.\n",
    "* Use no of component to 2 to easy visualization\n",
    "* Plot the PC1 and PC2 \n",
    "* Use different colors for class of wine\n",
    "* Do we need the target labels or y here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>1.3 Repeat section 1.1 but using PCA using Scikit-learn</font>\n",
    "\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform\n",
    "3. https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import PCA here \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# standardize the dataset, use link 1\n",
    "x = \n",
    "\n",
    "# Use pandas dataframe for easy handling\n",
    "X = pd.DataFrame(x, columns =  )\n",
    "# X.head()\n",
    "\n",
    "\n",
    "# for simplification lets Opt for only 2 components\n",
    "pca_sci = PCA()\n",
    "\n",
    "# use fit_transform\n",
    "PCs = pca_sci\n",
    "\n",
    "\n",
    "# pca has attribute that gives out the eigen values and its ratio. use dir(pca) to check the list of acceptable attribute\n",
    "print ()\n",
    "\n",
    "# Transforming all the array and list data into pandas for easy control\n",
    "PCs_df = pd.DataFrame(data = PCs, columns = ['PC 1', 'PC 2'])\n",
    "species = pd.DataFrame(wine.target, columns =['class'])\n",
    "PCs_df = pd.concat([PCs_df, species], axis = 1)\n",
    "print (PCs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can use the piece of code for ploting the scatter plot\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 Component PCA', fontsize = 20)\n",
    "\n",
    "targets = [0 , 1, 2]\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = PCs_df['class'] == target\n",
    "    ax.scatter(PCs_df.loc[indicesToKeep, 'PC 1'], PCs_df.loc[indicesToKeep, 'PC 2'], c=color, s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> 1.3.1 What percentage of variance does PC1 and PC2 together give? will only 2 component reproduces the original dataset faithfully?</font>\n",
    "\n",
    "Look into the attributes section: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Together, the first two principal components contain only %55.4 of the information. The first principal component contains 36.2% of the variance and the second principal component contains 19.2% of the variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>1.3.2 Without using loop, can we iterate the no of component and obtain their variance?</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify the code below\n",
    "pca1 = \n",
    "PCs = pca1.fit_transform()\n",
    "np.cumsum(pca1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>1.3.3 Choosing the right dimension </font> \n",
    "\n",
    "* we know that we can set the dimension in the PCA using 'n_components= 2 or 3 or n-1' where n is the no of dimension. we can fixate on the dimension by gauging using the scree or looking into variance ratio\n",
    "* there is another way to PCA; by chosing the desired variance. This is done by chosing the n_components< 1\n",
    "* Find how many components needed to preserve a variance of atleast 60, 75 and 95 % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 60% \n",
    "pca2 = PCA()\n",
    "X_reduced = \n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 75% \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 95% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LDA\n",
    "\n",
    "LDA features\n",
    "* Linear transformation technique\n",
    "* Also known as **Fisher's LDA**\n",
    "* used for feature extraction \n",
    "* increases computation efficiency\n",
    "* reduces overfitting\n",
    "* In contrast to PCA methodology i.e., finding orthogonal component axes with maximum variance , LDA works on finding feature subspace that optimise class separability\n",
    "* Supervised\n",
    "\n",
    "\n",
    "For internal working of LDA refer to section - Supervised data compression via Linear Discriminant Analysis (LDA), from textbook by Sebastian Raschka, 2015, \"Python Machine Learning\", Packt Publishing.\n",
    "\n",
    "Also see: \n",
    "\n",
    "http://scikit-learn.org/stable/modules/lda_qda.html\n",
    "\n",
    "http://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "Some version may not import using this line of code: \"from sklearn.lda import LDA\", instead import from sklearn.discriminant_analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> 2.1 Using links from the above, implement the LDA from scikit-learn module </font> \n",
    "\n",
    "* Perform using Singular Value Decomposition (**SVD**)\n",
    "* For simplicity chose two components\n",
    "* Implement first for wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = pd.DataFrame(wine.target, columns =['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# from sklearn.lda import LDA, use SVD\n",
    "lda = LDA()\n",
    "\n",
    "X_lda = lda.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA_df = pd.DataFrame(data = X_lda, columns = ['LD1', 'LD2'])\n",
    "LDA_df = pd.concat([LDA_df, species], axis = 1)\n",
    "print (LDA_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can use the same piece of code used above for PCA to plot scatte\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('LD1', fontsize = 15)\n",
    "ax.set_ylabel('LD2', fontsize = 15)\n",
    "ax.set_title('2 Component Linear Disciminant', fontsize = 20)\n",
    "\n",
    "targets = [0 , 1, 2]\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = LDA_df['class'] == target\n",
    "    ax.scatter(LDA_df.loc[indicesToKeep, 'LD1'], LDA_df.loc[indicesToKeep, 'LD2'], c=color, s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>2.2 Compare between PCA and LDA. Which method results in more fidelity response to Dataset with only two components?</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** LDA seems to produce good result with two components contributing to almost 99%.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Nonlinear Methods for Dimensionality reduction\n",
    "\n",
    "In the class you had learnt about the dimensionality reduction for nonlinear dataset. \n",
    "\n",
    "Refer to section 3.1 on Multidimensional scaling (**MDS**), section 3.2 for Manifold Learnings , particularly the Locally Linear Embedding (**LLE**), Isometric Feature Mapping (**ISOMAP**) and Hessian EigenMaps.\n",
    "\n",
    "We will now implement those nonlinear dimensionality reduction concept on a Nonlinear dataset. S-curve manifold.\n",
    "\n",
    "\n",
    "http://scikit-learn.org/stable/modules/manifold.html#manifold\n",
    "\n",
    "\n",
    "An example of application of manifold learning on digits dataset is in the link : http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Nonlinear methods studied in class to S-curve dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "Axes3D\n",
    "\n",
    "n_points = 5000\n",
    "X, color = datasets.samples_generator.make_s_curve(n_points, random_state=10)\n",
    "n_neighbors = 10\n",
    "n_components = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "plt.suptitle(\"S-Curve dataset\", fontsize=20)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax.view_init(10, -60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now have the S-curve dataset, apply the linear transformation methods like PCA and observe the significance of Nonlinear & geometric methods.\n",
    "\n",
    "### <font color='Red'>Do the following: </font>\n",
    "* ### <font color='Red'> Apply PCA for desired variance of 90% </font>\n",
    "* ### <font color='Red'> Apply PCA for n_component = 2 </font>\n",
    "* ### <font color='Red'> Apply LLE </font>\n",
    "* ### <font color='Red'> Apply ISOMAP </font>\n",
    "* ### <font color='Red'> Apply Hessian EigenMaps </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import PCA here \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# main the desired variance of 90%\n",
    "pca_S = \n",
    "Y = pca_S.\n",
    "\n",
    "\n",
    "print (Y.shape)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(Y[:, 0], np.zeros((n_points,1))+0.01, c=color, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "Y = pca.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note: \n",
    "### PCA is an unsupervised method and doesnot use class label to maximize the variance, which is not the case with LDA method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use no of neighbors 10 and no of component 2.\n",
    "\n",
    "Y = \n",
    "\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISOMAP\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = \n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.axis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian EigenMaps\n",
    "\n",
    "Hessian eigen maps can be realized from LLE. Refer to the below link.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = \n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra lab works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat the above PCA and LDA for Cancer dataset and apply SVM for the reduced Xtrain and Xtest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
