{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 - Unsupervised Learning\n",
    "## 10/25/2019\n",
    "\n",
    "In the Tuesday's class you had an opportunity to learn about the clustering, different methodologies and metrics for performance evaluation. \n",
    "\n",
    "* This lab session focuses on excercising unsupervised learning algorithm using Scikit-learn\n",
    "* We will also take look into Naive-Bayes classfiers\n",
    "\n",
    "\n",
    "Datasets: blobs, moons and UCI wine red dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rc('figure', figsize = (12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misc: (Might be of help for improving your plot visualization)\n",
    "From plt.style.available, we can print out all the available style packages. Use one of them here for matplotlib.\n",
    "\n",
    "https://tonysyu.github.io/raw_content/matplotlib-style-gallery/gallery.html\n",
    "\n",
    "To use in code: *plt.style.use('your selection here')*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prints all the available styles\n",
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "\"Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.  Bayes’ theorem states the following relationship, given class variable *y* and dependent feature vector *x1* through *xn*\"\n",
    "\n",
    "NB from Scikit-Learn:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "* Gaussian Naive Bayes (GaussianNB )\n",
    "* Multinomial Naive Bayes (MultinomialNB)\n",
    "* Bernoulli Naive Bayes (BernoulliNB )\n",
    "\n",
    "**Reference: **\n",
    "\n",
    "[1.] https://www.python-course.eu/naive_bayes_classifier_introduction.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "# from scikit learn import dataset, metrics and NB. Use link above for help.\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "# load the wine datasets\n",
    "\n",
    "# fit a Naive Bayes model to the data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and performance metric evaluation\n",
    "expected = \n",
    "\n",
    "predicted = \n",
    "\n",
    "# evaluate trained model using classification report and confusion matrix\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We might revisit this topic again during our lab on sentiment analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n",
    "### Read through section 7.2.3 of textbook\n",
    "\n",
    "useful links:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make use of the sklearn datasets - make blobs and moons\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "''' use make blobs with 150 samples, centers=3, features = 2, sd = 0.6, random state =  1'''\n",
    "X, y = make_blobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA \n",
    "''' visualize to see what the make-blobs has returned '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. k-means clustering\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "- class *sklearn.cluster.KMeans*\n",
    "- go through the parameters in the link \n",
    "\n",
    "\n",
    "$$arg min_c \\sum\\limits_{j=1}^k \\sum\\limits_{x=c_j} ( d(x, \\mu_j) = arg min_c \\sum\\limits_{j=1}^k \\sum\\limits_{x=c_j} \\big( ||x-\\mu_j||^2 \\big)^{1/2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Kmeans from sklearn, try with different values of cluster group (1-5)\n",
    "\n",
    "from \n",
    "\n",
    "''' Based on your understanding from the kmeans parameters in the link, select the parameters according but \n",
    "Make random_state = 0'''\n",
    "\n",
    "km = KMeans()\n",
    "\n",
    "y_km = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the cluster group and centroid computed from model\n",
    "* Coordinates of cluster centers can be obtained from the model (see [1])\n",
    "* What do you think will the be no of unclassified labels here ??\n",
    "\n",
    "[1.] http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' It is recommended to use different markers and colors for each class/clusters'''\n",
    "\n",
    "# Use scatter plot matrix to visualize the samples and the centroids\n",
    "plt.scatter()\n",
    "plt.scatter()\n",
    "plt.scatter()\n",
    "plt.scatter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since the no of samples in the observation is less and the centroid is placed sufficiently. \n",
    "* But in real dataset, we would not have a definitive answer\n",
    "* Hence, we need to evaluate the model and quantify the quality of clustering. From the textbook you would be introduced to different metrics of measure - Rand Index, Homogeniety, Completeness and V-measures, Silhouette scores. Some of these are straight forward.\n",
    "* For the above model, evaluate using within-cluster SSE (also known as distortion) and visualize it for different neighbors (1-10)\n",
    "* Similar try with Silhouette plots\n",
    "\n",
    "[1.] http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "[2.] http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Using for loop, fit the kmeans models for n-clusters from 1 to 10, let random state be 0, maxx iter = 100, n_init = 10.\n",
    "\n",
    "Step 1: initialize the model\n",
    "Step 2: fit the model\n",
    "Step 3: internal model of kmeans has one of the attributes as interia_, append these and plot after looped out.\n",
    "'''\n",
    "\n",
    "\n",
    "distortions = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans()\n",
    "    km.fit(X)\n",
    "    distortions.append()\n",
    "\n",
    "    \n",
    "plt.plot(range(1,11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette plots\n",
    "\n",
    "Check the performance of kMeans using Silhouette plots. Try for different values of n_clusters. When is it good and when does it go bad??\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''silhouette is to be imported from metrics package'''\n",
    "\n",
    "from sklearn.metrics import silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' The second link provided above, selecting the no of cluster i.e., unlabeled targets based on silhouette is discussed.\n",
    "further, in the below cell,initialize the model, fit and make necessaary changes to the code to display a plot'''\n",
    "km = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "cluster_labels = np.unique(y_km)\n",
    "n_clusters = cluster_labels.shape[0]\n",
    "silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" No modification needed\"\"\"\n",
    "\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "    c_silhouette_vals.sort()\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(i / n_clusters)\n",
    "    \n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color)\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "\n",
    "silhouette_avg = np.mean(silhouette_vals)\n",
    "plt.axvline(silhouette_avg,color=\"red\",linestyle=\"--\")\n",
    "plt.yticks(yticks, cluster_labels + 1)\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Silhouette coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' * Lets use make_moons.\n",
    "    * use sample size = 300, noise = 5%, random state = 0\n",
    "    * from link above for agc intialize the agglomerative clustering with no of cluster =2 and other parameters as necessary'''\n",
    " \n",
    "X, y = make_moons() # Enter parameters\n",
    "\n",
    "agc = AgglomerativeClustering() # Apply AGC here\n",
    "\n",
    "# use fit_predict and print the cluster output\n",
    "y_agc =         # fir_predict to get the yhat\n",
    "\n",
    "print('Cluster labels: %s' % y_agc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Similar to the kMeans, visulize the cluster type for the sample\"\"\"\n",
    "plt.scatter(X[y_agc==0,0], X[y_agc ==0,1], c='red', marker='s', label='cluster 1')\n",
    "plt.scatter(X[y_agc ==1,0],X[y_agc ==1,1], c='blue', marker='o', label='cluster 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot of half moons for both Kmeans and Agg clustering method, it can be observed that the clustering is not efficient. This is mainly becuase of the samples which has an orbitrary shapes. \n",
    "* DBSCAN \n",
    "* handles clustering data of arbitrary shapes efficiently\n",
    "* http://scikit-learn.org/stable/modules/clustering.html\n",
    "* http://scikit-learn.org/stable/modules/clustering.html#dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Import DBscan from sklearn, use eps = 0.2, min_samples = 5, metric = 'euclidean' '''\n",
    "\n",
    "\n",
    "import \n",
    "\n",
    "\n",
    "db = DBSCAN()\n",
    "y_db = db.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Doing same thing but for DBSCAN'''\n",
    "plt.scatter(X[y_db==0,0], X[y_db ==0,1], c='red', marker='s', label='cluster 1')\n",
    "plt.scatter(X[y_db ==1,0],X[y_db ==1,1], c='blue', marker='o', label='cluster 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating cluster performance for above models - kMeans, AGC and DBSCAN\n",
    "\n",
    "### http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Using piece of code from class, evaluate the performance of all the above clustering techniques'''\n",
    "# I have tried for kMeans. Do the same for other models!\n",
    "km = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "print ('Final evaluation of the clustering: K-Means')\n",
    "print('Inertia: %.2f' %  km.inertia_ )\n",
    "print('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y_km.ravel(),km.labels_))\n",
    "print('Homogeneity %.2f' %  metrics.homogeneity_score(y_km.ravel(),km.labels_))\n",
    "print('Completeness %.2f' %  metrics.completeness_score(y_km.ravel(),km.labels_))\n",
    "print('V_measure %.2f' %  metrics.v_measure_score(y_km.ravel(), km.labels_))\n",
    "print('Silhouette %.2f' %  metrics.silhouette_score(X, km.labels_, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. WINE DATASET\n",
    "\n",
    "From UCI repository, download the wine dataset and apply the above NB classifiers, k-Means and HCA \n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Lets try on the real dataset.\n",
    "we have used the wine dataset earlier. we would be using this for clustering application and since we know that the target group \n",
    "is the wine quality and is a part of this dataset in the link'''\n",
    "\n",
    "'''From the url given, import the dataset using pd.read_csv,\n",
    "make necessary changes for successfully reading this data\n",
    "update the column names with the data description given in the UCI link'''\n",
    "\n",
    "url  =  \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\" \n",
    "wine = pd.read_csv(url,sep = ',', header=None)\n",
    "wine.columns = ['class', 'Alcohol','Malic acid','Ash','Alcalinity of ash' ,'Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','color intensity','Hue','OD280/OD315 of diluted wines','Proline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' since, target class is also part of the data, slice the data and assign to y and remaining data to variable X'''\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the class/target below\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram plot of one of the attribute\n",
    "sns.set()\n",
    "\n",
    "for i in np.unique(y):\n",
    "    sns.distplot(wine['Malic acid'][y==i], kde=1,label='{}'.format(i))\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Apply Naive Bayes classification to this and check if the classification matches the target class ?'''\n",
    "# Gaussian Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using either confusion matrix or classification report, see which one seems to be apt in NB ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' We have to evaluate the model inorder to gauge the parameter selected ofcourse. Since, this is a classification, \n",
    "we can use classification report from metrics and/or confusion matrix'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Use the excercises above for wine dataset and apply kMeans , AGC for wine dataset'''\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### As done in class, use those piece of code to produce a dendogram plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "dist = pdist(X,'euclidean')\n",
    "linked = linkage(X, 'single')\n",
    "# linkage_matrix = linkage(dist, method = 'complete');\n",
    "plt.figure()  # we need a tall figure\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((15,15))\n",
    "dendrogram(linked, orientation=\"top\", color_threshold = 10, distance_sort='descending', show_leaf_counts=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap and dendrogram can be produced using seaborn\n",
    "\n",
    "* https://seaborn.pydata.org/generated/seaborn.clustermap.html\n",
    "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
