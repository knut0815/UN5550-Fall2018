{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Regression Analysis (Chapter 6)\n",
    "\n",
    "This lab session focuses on a regression analysis and its applications. We will excercise some of the python tools for building and evaluating the regression models. \n",
    "\n",
    "Specifically we will look into:\n",
    "* Simple Linear Regression \n",
    "* Multiple Linear Regression\n",
    "* Polynomial Regression \n",
    "* Logistic Regression\n",
    "* Pickling \n",
    "\n",
    "Before, getting into lab excercise, we will look into the basics and run codes for regression models from textbook\n",
    "\n",
    "For future reference: \n",
    "\n",
    "* http://scikit-learn.org/stable/modules/linear_model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models\n",
    "* ** *Regression models are used to predict target variables on a continuous scale, which makes them attractive for addressing many questions in science as well as applications in industries. They are used to predict the trends, forecast the future, relationship between the variables* ** (source from Python Machine Learning by Sebastian Raschka) \n",
    "* The main concepts while building the regession models are:\n",
    "        - Exploring and visualizing the dataset (EDA)\n",
    "        - Selection of regression models \n",
    "        - Training regression model \n",
    "        - Evaluating the model built \n",
    "        - Fitting these models to dataset under study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "The simplest relation that can exist in the model is the **linear model**, where the response $\\textbf{y}$ (response or target variable) depends linearly from the covariates $\\textbf{x}_i$ (feature).\n",
    "\n",
    "In the **simple** linear regression, with a single variable, we described the relationship between the predictor and the response with a straight line. The model is:\n",
    "$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 $$\n",
    "\n",
    "The parameter \n",
    "\n",
    "$a_0$ is called the constant term or the *intercept*.\n",
    "\n",
    "$a_1$ is called the coefficient of the explanatory variable *coeff*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rc('figure', figsize = (10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to observe the linear regression using the normal distributed dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.randn(300, 2)  # Random points sampled from a univariate “normal” (Gaussian) distribution\n",
    "A = np.array([[0.6, .4], [.4, 0.6]])\n",
    "X2 = np.dot(X1, A) # np.dot() returns the dot product of two array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualizing this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(X2[:, 0]).hist(histtype='stepfilled', alpha=.5, bins=20)\n",
    "plt.xlabel('random obs',fontsize=12)\n",
    "plt.ylabel('Samples of X',fontsize=12)\n",
    "pd.DataFrame(X2[:,1]).hist(histtype='stepfilled', alpha=.5, color=sns.desaturate(\"indianred\", .75), bins=20)\n",
    "plt.xlabel('random obs',fontsize=12)\n",
    "plt.ylabel('Samples of Y',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot of the observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X2[:, 0], X2[:, 1],\"o\", alpha = 0.5, c='g') # alpha, blending value, between 0 (transparent) and 1 (opaque)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The redline indicates the regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [0+1*x for x in np.arange(-2,3)]\n",
    "plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha = 0.4, c='g');\n",
    "plt.plot(np.arange(-2,3), model, 'r');\n",
    "plt.show()\n",
    "# The red line gives the predicted values of this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There could be multiple regression models (in this case, multiple fitting lines), similar to the classifiers, we need to gauage and evaluate. \n",
    "* The best fitting line is known as regression line \n",
    "* Another important terminalogy is the **residue** or **offsets** which is nothing but an errors of the prediction with the real parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha = 0.3);\n",
    "# We can use several parameters and we do not know which is the best model\n",
    "model1 = [0.0 + 1.0*x for x in np.arange(-2,3)]\n",
    "model2 = [0.3 + 0.9*x for x in np.arange(-2,3)]\n",
    "model3 = [0.0 - 0.1*x for x in np.arange(-2,3)]\n",
    "plt.plot(np.arange(-2,3), model1, 'r')\n",
    "plt.plot(np.arange(-2,3), model2, 'g')\n",
    "plt.plot(np.arange(-2,3), model3, 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares\n",
    "\n",
    "Ordinary Least Squares (OLS) is the simplest and most common **estimator** in which the coefficients $a$'s \n",
    "of the simple linear regression: $\\textbf{y} = a_0+a_1 \\textbf{x}$, \n",
    "are chosen to minimize the **square of the distance between the predicted values and the actual values**. \n",
    "\n",
    "Given the set of samples $(\\textbf{x},\\textbf{y})$, the objective is to minimize:\n",
    "\n",
    "$$ ||a_0 + a_1 \\textbf{x} -  \\textbf{y} ||^2_2 = \\sum_{j=1}^n (a_0+a_1 x_{j} -  y_j )^2,$$ with respect to $a_0, a_1$.\n",
    "\n",
    "This expression is often called **sum of squared errors of prediction (SSE)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### We will run an example case from text book and analyze the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading Sea Ice Data and climate change dataset\n",
    "* Dataset: **SeaIce.txt **\n",
    "* Preprocessing and Data Cleaning is necessary\n",
    "* Processed data is to be stored as a pickle (*.pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ice = pd.read_csv('SeaIce.txt', delim_whitespace = True)\n",
    "print ('shape:', ice.shape)\n",
    "ice.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice.mean() \n",
    "# Observation: a negative mean?!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "x = ice.year\n",
    "y = ice.extent\n",
    "plt.scatter(x, y, color = 'red')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Extent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation from above: \n",
    "    \n",
    "    - We see that there is an anamoly in the dataset\n",
    "    - \"-9999\" does not seems to be a data, this value has potential to impact the regression line\n",
    "    - One has to be very careful in analyzing this type of outliers. This is where, EDA (Exploratory Data Analysis) comes        into picture\n",
    "    - Data Cleaning and Pre-processing is very much essential in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Different values in data_type field:\\n', np.unique(ice.data_type.values) , '\\n')  # there is a -9999 value!)\n",
    "# Let's see what type of data we have more than Goddard and NRTSI-G \n",
    "print (ice[(ice.data_type != 'Goddard')\n",
    "          & (ice.data_type != 'NRTSI-G')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily clean the data now:\n",
    "ice2 = ice[ice.data_type != '-9999']\n",
    "print ('shape:', ice2.shape)\n",
    "# And repeat the plot\n",
    "x = ice2.year\n",
    "y = ice2.extent\n",
    "plt.scatter(x, y, color = 'red')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Extent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PICKLE \n",
    "\n",
    "* Standard library module \n",
    "* pickling/unpickling (serialization/de-serialization) - converts the python modules into byte stream and to back.\n",
    "* Very efficient \n",
    "\n",
    "Please read through the below link: \n",
    "\n",
    "https://docs.python.org/3/library/pickle.html#module-pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a pickle and sending in the dataframe of ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in the pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://seaborn.pydata.org/generated/seaborn.lmplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"mo\", \"extent\", SeaIce, size = 5.2, aspect = 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean for each month.\n",
    "grouped = SeaIce.groupby('mo')\n",
    "month_means = grouped.extent.mean()\n",
    "month_variances = grouped.extent.var()\n",
    "print ('Means:', month_means)\n",
    "print ('Variances:',month_variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "for i in range(12):\n",
    "    SeaIce.extent[SeaIce.mo == i+1] = 100*(SeaIce.extent[SeaIce.mo == i+1] - month_means[i+1])/month_means.mean()\n",
    "    \n",
    "sns.lmplot(\"mo\", \"extent\", SeaIce, size = 5.2, aspect = 2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"year\", \"extent\", SeaIce,size = 5.2, aspect = 2);\n",
    "# plt.savefig(\"files/ch06//IceExtentAllMonthsByYearlmplot.png\", dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"year\", y=\"extent\", hue=\"mo\", col=\"mo\", data=SeaIce, col_wrap=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the trend as a simple linear regression (OLS) and quantitatively evaluate it.\n",
    "\n",
    "For that we use using **Scikit-learn**, library that provides a variety of both supervised and unsupervised machine learning techniques.\n",
    "Scikit-learn provides an object-oriented interface centered around the concept of an Estimator. \n",
    "The <code>Estimator.fit</code> method sets the state of the estimator based on the training data. Usually, the data is comprised of a two-dimensional numpy array $X$ of shape <code>(n_samples, n_predictors)</code> that holds the so-called feature matrix and a one-dimensional numpy array $\\textbf{y}$ that holds the responses. Some estimators allow the user to control the fitting behavior. \n",
    "For example, the <code>sklearn.linear_model.LinearRegression</code> estimator allows the user to specify whether or not to fit an intercept term. This is done by setting the corresponding constructor arguments of the estimator object.\n",
    "During the fitting process, the state of the estimator is stored in instance attributes that have a trailing underscore ('_'). For example, the coefficients of a LinearRegression estimator are stored in the attribute coef_.\n",
    "\n",
    "Estimators that can generate predictions provide a ``Estimator.predict`` method. \n",
    "In the case of regression, ``Estimator.predict`` will return the predicted regression values, $\\hat{\\textbf{y}}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets use the Linear Regression models from sklearn linear models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "est = LinearRegression(fit_intercept = True)\n",
    "\n",
    "x = SeaIce[['year']]\n",
    "y = SeaIce[['extent']]\n",
    "\n",
    "est.fit(x, y)\n",
    "\n",
    "print (\"Coefficients:\", est.coef_)\n",
    "print (\"Intercept:\", est.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We can evaluate the model fitting by computing**\n",
    "\n",
    "    i. Mean Squared Error ($MSE$) \n",
    "\n",
    "    ii. Coefficient of determination ($R^2$) \n",
    "\n",
    "The coefficient $R^2$ is defined as \n",
    "\n",
    "$(1 - \\textbf{u}/\\textbf{v})$,\n",
    "\n",
    "where $\\textbf{u}$ is the residual sum of squares $\\sum (\\textbf{y} - \\hat{\\textbf{y}})^2$ and $\\textbf{v}$ is the regression sum of squares $\\sum (\\textbf{y} - \\bar{\\textbf{y}})^2$, where $\\bar{\\textbf{y}}$ is the mean.\n",
    "\n",
    "The best possible score for $R^2$ is 1.0: lower values are worse.\n",
    "\n",
    "These measures can provide a quantitative answer to the question we are facing: Is there a negative trend in the evolution of sea ice extent over recent years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Analysis for all months together.\n",
    "x = SeaIce[['year']]\n",
    "y = SeaIce[['extent']]\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "y_hat = model.predict(x)\n",
    "plt.plot(x, y,'o', alpha = 0.5)\n",
    "plt.plot(x, y_hat, 'r', alpha = 0.5)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('extent (All months)')\n",
    "\n",
    "# plt.savefig(\"files/ch06/IceExtentLinearRegressionAllMonthsByYearPrediction.png\", dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise: From the current understanding, plot a residuals for the above plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to evaluate the performance of linear regression is to use MSE and R2 as a measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From Sklearn metrics import MSE and R-square methods and apply to the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear & Polynomial Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its name, linear regression can be used to fit non-linear functions. A linear regression model is linear in the model parameters, not necessarily in the predictors. If you add non-linear transformations of your predictors to the linear regression model, the model will be non-linear in the predictors.\n",
    "\n",
    "A very popular non-linear regression technique is *Polynomial Regression*, a technique which models the relationship between the response and the predictors as an n-th order polynomial. \n",
    "\n",
    "$$ \\textbf{y} = a_1 \\phi(\\textbf{x}_1) + \\dots + a_m \\phi(\\textbf{x}_m) $$\n",
    "\n",
    "The higher the order of the polynomial the more \"wigglier\" functions you can fit. \n",
    "\n",
    "Using higher order polynomial comes at a price: **computational complexity** and **overfitting**. Overfitting refers to a situation in which the model fits the idiosyncrasies of the training data and loses the ability to generalize from the seen to predict the unseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston data set\n",
    "\n",
    "**Attributes**\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per 10,000 dollars\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X_boston,y_boston = boston.data, boston.target\n",
    "print ('Shape of data:', X_boston.shape, y_boston.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "df_boston['PRICE'] = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot of price vs Average no of rooms per dwelling\n",
    "* use lmplot (price and RM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the relations between price and RM\n",
    "sns.lmplot(\"PRICE\",\"RM\", df_boston, size = 5.2,aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot of price vs  % lower status of the population\n",
    "* Use lmplot (Price and Lstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the relations between price and LSTAT\n",
    "sns.lmplot(\"PRICE\", \"LSTAT\", df_boston, size = 5.2, aspect = 2)\n",
    "# plt.savefig(\"files/ch06/lmplotBostonLSTAT.png\", dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "x = df_boston[['PRICE']]\n",
    "y = df_boston[['LSTAT']]\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "y_hat = model.predict(x)\n",
    "\n",
    "print (\"MSE:\", metrics.mean_squared_error(y_hat, y))\n",
    "print (\"R^2:\", metrics.r2_score(y_hat, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We see that the regression line is poorly fit and which causes large MSE and low R2 score **\n",
    "\n",
    "** We can now look into multiple linear regression and polynomial linear regression **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise: \n",
    "\n",
    "Apply different order to the linear regression and observe the plot.\n",
    "* Let interested variable be LSTAT and price.\n",
    "* Multiple linear regression can be realised using the order in lmplot\n",
    "* Build the multiple linear regression \n",
    "* Try changing the order to 2, 3, 5 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying linear regression and finding the relationship for each variable is a time-consuming. There are various functions that help to analyze the complete dataset and its relation. A few of them are\n",
    "* Correlation (corr) - Used to compute the Correlation between variable. Available in pandas 'corr'. Various correlation methods can be applied - pearson, spearman and kendall. - https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html\n",
    "* Heatmap - https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
    "* Scatterplot matrix - https://seaborn.pydata.org/examples/scatterplot_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap \n",
    "indexes = [0, 2, 4, 5, 6, 12] #To plot just some of the variables\n",
    "df2 = pd.DataFrame(boston.data[:, indexes], columns = boston.feature_names[indexes])\n",
    "df2['price'] = boston.target\n",
    "corrmat = df2.corr(method='spearman')\n",
    "\n",
    "# Draw the heatmap using seaborn\n",
    "sns.heatmap(corrmat, square = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see the LSTAT and Price has a largest correlation of -0.85 and from Scatter plot, it is evident that the data is nonlinear. Some more EDA are Scatter Matrix Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [5, 6, 12]\n",
    "df2 = pd.DataFrame(boston.data[:,indexes], columns = boston.feature_names[indexes])\n",
    "df2['PRICE'] = boston.target\n",
    "pd.scatter_matrix(df2, figsize = (12.0, 12.0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Seaborn, we can also fit linear regression models to the scatter matrix plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df2, kind=\"reg\", aspect=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise:\n",
    "        - Split the boston dataset into test and training (50:50)\n",
    "        - Apply Linear Regression\n",
    "        - Find the coefficient and intercepts of the predicted model\n",
    "        - Compute testing and training scores \n",
    "        - Find MSE\n",
    "        - use variables by name - X_train, X_test, y_train, y_test, X_boston,y_boston, \n",
    "        \n",
    "        \n",
    "* For MSE, refer to http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
    "* Applying Linear Regression and finding coefficient, look into : http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting linear regression\n",
    "regr_boston = \n",
    "\n",
    "# Train the model\n",
    "regr_boston.\n",
    "\n",
    "print ('Coeff and intercept:', regr_boston.coef_, regr_boston.intercept_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best possible score is 1.0, lower values are worse. Print out the performance using the score, MSE and R2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the below code to visualize the residuals of linear regression model\n",
    "### Is this the best fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train, (regr_boston.predict(X_train)-y_train), c='blue', marker='o', label='Training Data', alpha = 0.8)\n",
    "plt.scatter(y_test, (regr_boston.predict(X_test)-y_test), c='lightgreen', marker='s', label='Testing Data', alpha = 0.5)\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upperleft')\n",
    "plt.hlines(y=0, xmin=-10, xmax=60, lw=2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the polynomial regression for training data and observe the performance index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "quadratic = PolynomialFeatures(degree=2)\n",
    "Xtrain_quad = quadratic.fit_transform(X_train)\n",
    "regr_boston= regr_boston.fit(Xtrain_quad, y_train)\n",
    "ypredict_quad = regr_boston.predict(Xtrain_quad);\n",
    "print('R2 score:', metrics.r2_score(y_train, ypredict_quad))\n",
    "print('MSE:', metrics.mean_squared_error(y_train, ypredict_quad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "cubic = PolynomialFeatures(degree=3)\n",
    "Xtrain_cubic = cubic.fit_transform(X_train)\n",
    "regr_boston= regr_boston.fit(Xtrain_cubic, y_train)\n",
    "ypredict_cubic = regr_boston.predict(Xtrain_cubic);\n",
    "print('R2 score:', metrics.r2_score(y_train, ypredict_cubic))\n",
    "print('MSE:', metrics.mean_squared_error(y_train, ypredict_cubic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion (Scikit-learn)\n",
    "\n",
    "By definition - **Logistic regression** is a type of probabilistic statistical classification model and is mainly used to predict a binary response from a binary predictor. \n",
    "\n",
    "* Also known as Logit Regression \n",
    "* It is used to estimate the probability that an instance belong to particular class or group\n",
    "* Binary classifiers \n",
    "* Typical instance used is - **win/loss, spam/not spam, positive/negative sentiment, 0/1 **\n",
    "* Used for predicting the outcome of a categorical dependent variable (i.e., a class label) based on one or more predictor variables (features). \n",
    "\n",
    "\n",
    "The logistic function is:\n",
    "\n",
    "$$ f(x) = \\frac{1}{1+e^{- \\lambda x}}$$\n",
    "\n",
    "The logistic function is useful because it can take an input with any value from negative infinity to positive infinity, whereas the output  is confined to values between 0 and 1 and hence is interpretable as a probability.\n",
    "\n",
    "** Y_p =  0 if p < 0.5 **\n",
    "\n",
    "** Y_p =  1 if p>= 0.5 **\n",
    "\n",
    "\n",
    "Ref: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logist(x,l):\n",
    "    return 1/(1+np.exp(-l*x))\n",
    "\n",
    "x = np.linspace(-10, 10) # 50 points equally spaced from -10 to 10\n",
    "t = logist(x, 0.5)\n",
    "y = logist(x, 1)\n",
    "z = logist(x, 3)\n",
    "plt.plot(x, t, label = 'lambda=0.5')\n",
    "plt.plot(x, y, label = 'lambda=1')\n",
    "plt.plot(x, z, label = 'lambda=3')\n",
    "plt.legend(loc = 'upper left')\n",
    "#plt.savefig(\"files/ch06/LogisticRegression.png\", dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Season 2012/2013\n",
    "file = 'SP1.csv' # file = 'http://www.football-data.co.uk/mmz4281/1213/SP1.csv'\n",
    "data_football = pd.read_csv(file)\n",
    "s = data_football[['HomeTeam','AwayTeam', 'FTHG', 'FTAG', 'FTR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualization with scatter the number of goals\n",
    "# plt.scatter(s.FTHG, s.FTAG, s = 100, alpha = 0.05) \n",
    "# # s, size of the points, alpha, blending value, between 0 (transparent) and 1 (opaque).\n",
    "# plt.xlabel('Home team goals (FTHG)')\n",
    "# plt.ylabel('Away team goals (FTAG)')\n",
    "# Create two extra columns containing 'W' the number of goals of the winner and 'L' the number of goals of the losser\n",
    "def my_f1(row):\n",
    "    return max(row['FTHG'], row['FTAG'])\n",
    "\n",
    "def my_f2(row):\n",
    "    return min(row['FTHG'], row['FTAG'])\n",
    "\n",
    "# Add 2 new columns to the panda:\n",
    "s['W'] = s.apply(my_f1, axis = 1)\n",
    "s['L'] = s.apply(my_f2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data and target\n",
    "import numpy as np\n",
    "x1 = s['W'].values \n",
    "y1 = np.ones(len(x1), dtype = np.int)\n",
    "x2 = s['L'].values \n",
    "y2 = np.zeros(len(x2), dtype = np.int)\n",
    "\n",
    "x = np.concatenate([x1, x2])\n",
    "x = x[:, np.newaxis]\n",
    "y = np.concatenate([y1, y2])\n",
    "\n",
    "\n",
    "def lr_model(x):\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting of a Logistic Regression and prediction using the model:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x, y)\n",
    "\n",
    "X_test = np.linspace(-5, 10, 300)\n",
    "\n",
    "loss = lr_model(X_test * logreg.coef_ + logreg.intercept_).ravel()\n",
    "\n",
    "X_test2 = X_test[:, np.newaxis]\n",
    "losspred = logreg.predict(X_test2)\n",
    "\n",
    "plt.scatter(x.ravel(), y, color = 'black', s = 100, zorder = 20, alpha = 0.03)\n",
    "plt.plot(X_test, loss, color = 'blue', linewidth = 3)\n",
    "plt.plot(X_test, losspred, color = 'red', linewidth = 3)\n",
    "plt.xlabel('Number of goals')\n",
    "plt.ylabel('Victory (1) or Defeat (0)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can build a classifier using the logistic regression \n",
    "* Detect the species type based on petal width\n",
    "* Import logistic regression from sklearn linear model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import iris dataset to iris\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the petal width as feature and assign to X \n",
    "X =                     # petal width\n",
    "\n",
    "# lets train the model to identify if target model is of Virginica species or not (Remember log reg is \n",
    "# used as a binary classifier) (also convert to integer) and assign to y\n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logistic regression from sklearn\n",
    "\n",
    "\n",
    "# assign the logistic regression to the below variable\n",
    "\n",
    "\n",
    "# training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the below array of petal width, plot an estimated probability curves  \n",
    "X_new = np.linspace(0, 3, 100).reshape(-1, 1)\n",
    "\n",
    "# hint: pass Xnew to trained model and observe the probability  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For Petal width above 2 cm what class of species does it belong to ? \n",
    "* Similarly if below 1 cm, what is the estimated species class ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
