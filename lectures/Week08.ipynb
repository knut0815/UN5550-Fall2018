{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "When we are given labels or target values associated with the training and test data, we apply supervised learning techniques to generate models that match target values or labels as precisely as possible.  When no such labels or target values are available, the best we can do is seek hidden structure in the data.  Unsupervised learning algorithms are designed to learn from features to categorize these unlabeled data.  \n",
    "\n",
    "Goals for this module:\n",
    "* Geometric distances\n",
    "* Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Picture\n",
    "\n",
    "What are we trying to do?  Discover clusters/categories without labels.  Idea:\n",
    "* samples within a cluster should be \"similar\"\n",
    "* samples that belong to different clusters, they should be \"dissimilar\"\n",
    "\n",
    "Usual steps for clustering:\n",
    "1. __Data representation__: includes preparation and initial work, such as choosing the number of clusters to look for, picking what measurements to use, how many observations to process, choosing scaling and other transformations of the data\n",
    "2. __Proximity Measure__: many clustering methods require a measure of distance or proximity between observations, and maybe between clusters. \n",
    "3. __Grouping__: Process of partitioning the data into clusters.  This grouping can be \"hard\", meaning that an observation either belongs to a group, or it does not.  This grouping can also be \"fuzzy\" or \"soft\", where each sample has a degree of membership in each cluster.  The grouping can also be hierarchical, where we have a nested sequence of partitions.\n",
    "4. __Data abstraction__: optional process of obtaining a simple and compact representation of the partitions, for example, this cluster could represent the set of data science students at Michigan Tech.\n",
    "5. __Clusters Assessment__:are the clusters meaningful\n",
    "\n",
    "\n",
    "Some natural questions that are relevant to clustering:\n",
    "* how do we quantify this notion of similar and dissimilar?\n",
    "* what algorithms should we use to cluster the data?  fast/efficient? deterministic/stochastic? \n",
    "* how many clusters do we look for?  can it be adaptively chosen?  \n",
    "* do we allow very large and very small clusters?\n",
    "    * which algorithms work best for a large number of samples?\n",
    "    * which algorithms work best when there are many classes?\n",
    "* how do we quantify how the performance of our clustering/grouping?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Distance\n",
    "\n",
    "We need some measure of distance between samples. If the features are continuous variables, a popular choice is the Euclidean distance.  Let $\\vec{x}$ and $\\vec{y}$ be two observations, each consisting of $D$ continuous features, i.e., $\\vec{x},\\vec{y}\\in\\mathbb{R}^D$.  Then, the Euclidean distance is defined as \n",
    "\\begin{align}\n",
    "d(\\vec{x},\\vec{y}) = \\|\\vec{x} - \\vec{y}\\|_2 =  \\left( \\sum_{i=1}^D (x_i - y_i)^2 \\right)^{1/2}\n",
    "\\end{align}\n",
    "We could also weight the distances using:\n",
    "\\begin{align}\n",
    "d(\\vec{x},\\vec{y}) = \\|\\vec{x} - \\vec{y}\\|_{w,2} =  \\left( \\sum_{i=1}^D w_i(x_i - y_i)^2 \\right)^{1/2}\n",
    "\\end{align}\n",
    "\n",
    "Other options include the Manhattan distance, which you will explore in your project,\n",
    "\\begin{align}\n",
    "d(\\vec{x},\\vec{y}) = \\|\\vec{x} - \\vec{y}\\|_1 = \\sum_{i=1}^D |x_i - y_i|\n",
    "\\end{align}\n",
    "or the maximum distance,\n",
    "\\begin{align}\n",
    "d(\\vec{x},\\vec{y}) = \\|\\vec{x} - \\vec{y}\\|_\\infty = \\max_i |x_i - y_i|\n",
    "\\end{align}\n",
    "\n",
    "Some properties of distance measures:\n",
    "* $d(\\vec{x},\\vec{y}) \\ge 0$\n",
    "* $d(\\vec{x},\\vec{x}) = 0 $\n",
    "* $d(\\vec{x},\\vec{y}) = d(\\vec{y},\\vec{x})$\n",
    "* $d(\\vec{x},\\vec{y}) \\le d(\\vec{x},\\vec{z}) + d(\\vec{z},\\vec{y})$ (Triangle Inequality)\n",
    "\n",
    "With $N$ samples, one can generate a distance matrix, $D$, which is of size $n\\times n$, and whose elements $d_{ij}$ correspond to the distance between sample $\\vec{x}_i$ and sample $\\vec{x}_j$, i.e. $d_{ij} = d(\\vec{x}_i,\\vec{x}_j)$.\n",
    "\n",
    "If a distance measure was available, one can measure similarity by using a kernel.  Most commonly, the Gaussian kernel is used,\n",
    "\\begin{align}\n",
    "  s(\\vec{x},\\vec{y}) = \\exp \\left(-\\gamma\\,d(\\vec{x},\\vec{y}) \\right)\n",
    "\\end{align}\n",
    "\n",
    "Why do we call this a similarity measure?  If $d(\\vec{x},\\vec{y}) = 0$, then $s(\\vec{x},\\vec{y})$, the similarity, is equal to 1 (the maximum similarity possible). If $d(\\vec{x},\\vec{y})$ is very large, then the similarity is very small.  One can choose whether to use the distance measure or the similarity measure to try and cluster the data. \n",
    "\n",
    "Note: scaling the data (either by pre-processing, or by using a weighted distance measure) can have significant impact on the data.  Consider the following simple example of four people where we have their age (in years) and their height (in feet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "population = {'height (feet)': [5, 6, 5, 6], 'age': [25, 25, 30, 30]}\n",
    "df = pd.DataFrame(data = population)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = df.plot.scatter(x='age', y = \"height (feet)\")\n",
    "_ = ax1.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "might lead us to the conclusion that there are two clusters, and we should group people with the same age together.  However, if we lived outside the US, we would normally report the height in centimeters.  Lets create a new column in our data frame and generate a new scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"height (cm)\"] = df[\"height (feet)\"]*12*2.54\n",
    "df\n",
    "\n",
    "ax1 = df.plot.scatter(x='age', y = \"height (cm)\")\n",
    "_ = ax1.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we might conclude that there are two clusters, and we should group people with the same height together. This should serve as a warning that scaling (weighting) plays a crucial role in the generation of the clusters.  There is no single \"correct\" answer as to whether to scale or not.  Some observations though:\n",
    "* if variables are not scaled, the variable with the largest range has the most weight in the distance.\n",
    "* scaling gives every variable equal weight\n",
    "* don't scale if the units are the same for all features, e.g., every feature is some measurement in pounds.\n",
    "* scaling is okay if the variables measure different units (e.g., age in years, height in cm).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we do if some of the data is categorical (a.k.a. mixed data?)  \n",
    "* we could transform the categorical data into a continuous equivalent...  This is tricky because scaling might unfairly favor a feature.  \n",
    "* can use an alternative formulation for similarity: most popular one is the Gower dissimilarity metric. The categorical variables are expressed in binary columns based on which category they fall into.  Each variable is then standardized (subtract mean, divide by standard deviation.  The categorical columns are further divided by $\\sqrt{2}$ to compensate for their 0/1 encoding.  \n",
    "* The last two pages of these notes http://www.econ.upf.edu/~michael/stanford/maeb5.pdf gives a nice example of how to compute the Gower dissimilarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "### Rand Index\n",
    "Before diving into clustering algorithms,we need to understand how to compare clustering results since there are no labels to measure accuracy of our clustering.  Suppose we have $n$ samples in a set $S$, and \n",
    "* clustering method \\#1 partitions $S$ into $r$ subsets, $X = \\{X_1, \\ldots, X_r\\}$,\n",
    "* clustering method \\#2 partitions $S$ into  $s$ subsets, $Y = \\{Y_1, \\ldots, Y_s\\}$.\n",
    "\n",
    "Define the following:\n",
    "\n",
    "* a, the number of pairs of elements in S that are in the same set in X and in the same set in Y\n",
    "* b, the number of pairs of elements in S that are in different sets in X and in different sets in Y\n",
    "* c, the number of pairs of elements in S that are in the same set in X and in different sets in Y\n",
    "* d, the number of pairs of elements in S that are in different sets in X and in the same set in Y.\n",
    "\n",
    "The Rand index, R, is:\n",
    "$ R = \\frac{a+b}{a+b+c+d} = \\frac{a+b}{{n \\choose 2 }}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is easier seen in an example.  \n",
    "* Suppose we have six samples in our set: $\\{a, b, c, d, e, f\\}$. \n",
    "* Clustering method 1 (CM1) forms three clusters; \n",
    "    * $\\{a,b\\}$ are in group 0, \n",
    "    * $\\{c,d\\}$ are in group 1, \n",
    "    * $\\{e,f\\}$ are in group 2\n",
    "* Clustering method 2 (CM2) forms two clusters; \n",
    "    * $\\{a,b,c\\}$ are in group 0\n",
    "    * $\\{d,e,f\\}$ are in in group 1\n",
    "    \n",
    "How do these two clustering methods compare?  What is the Rand index for these two clustering results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the Rand index, we need to exhaustively check every unordered pair.  There are 15 unordered pairs from the set of six samples:\n",
    "$\\{a, b\\}, \\{a, c\\}, \\{a, d\\}, \\{a, e\\}, \\{a, f\\}, \\{b, c\\}, \\{b, d\\}, \\{b, e\\}, \\{b, f\\}, \\{c, d\\}, \\{c, e\\}, \\{c, f\\}, \\{d, e\\}, \\{d, f\\}$, and $\\{e, f\\}$.\n",
    "\n",
    "* Of these 15 pairs, which ones are in the same clusters proposed by CM1 and CM2?\n",
    "In this example, only two: $\\{a, b\\}$ and $\\{e, f\\}$, so one variable for the rand score is $a=2$.\n",
    "* Of these 15 pairs, which ones are not grouped together by the two clustering methods?\n",
    "$\\{a, d\\}, \\{a, e\\}, \\{a, f\\}, \\{b, d\\}, \\{b, e\\}, \\{b, f\\}, \\{c, e\\}$, and $\\{c, f\\}$, \n",
    "so the second variable for the rand score is $b=8$.\n",
    "\n",
    "Thus, the rand score is : \n",
    "\n",
    "$ R = \\frac{a+b}{{n \\choose 2 }} = \\frac{10}{6 \\choose 2} = \\frac{10}{15} = 0.667$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python actually does not have a built-in method for computing the Rand index.  Instead, there are functions to compute an adjusted rand score.  The adjusted Rand index was designed to account for chance grouping of elements.  Specifically,\n",
    "\n",
    "* when given two clusterings with random labels, the adjusted rand index will return a value close to 0.\n",
    "\n",
    "* when the number of clusters increases, the adjusted rand index will return a number close to 1.\n",
    "\n",
    "How is the adjusted rand index (AR) computed?\n",
    "\n",
    "\\begin{align}\n",
    "\\text{AR} = \\frac{{n\\choose 2} (a+d) -  {\\large [}(a+b)(a+c) + (c+d)(b+d){\\large]}}{{n \\choose 2}^2 {\\large [}(a+b)(a+c) + (c+d)(b+d){\\large]}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our toy example, lets finish computing the remaining variables to compute the adjusted rand index.\n",
    "\n",
    "* Of these 15 pairs, only one pair, $\\{c, d\\}$ is in CM1 and not in CM2, so $c=1$.\n",
    "* Of these 15 pairs, four pairs are in different subsets of CM1, but in the same subset in Y, $\\{a,c\\}, \\{b,c\\},\\{d,e\\}$ and $\\{d,f\\}$, so $d = 4$.\n",
    "\n",
    "For our toy example, the adjusted rand index should be:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{AR} &= \\frac{ 15\\cdot(2+4) - \\left[(2+8)\\cdot(2+1) + (1+4)\\cdot(8+4)\\right]}{15^2 \\left[(2+8)\\cdot(2+1) + (1+4)\\cdot(8+4)\\right]}\\\\\n",
    "&=\\frac{90 - (30 +60)}{15^2\\cdot 90} = 0???\n",
    "\\end{align}\n",
    "\n",
    "not sure what I did wrong...  couldn't find the textbook definition of adjusted rand index elsewhere in a brief search.  In any case, there is a built in function in scikit-learn that computes the adjusted rand score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "adjusted_rand_score([0, 0, 1, 1, 2, 2], [0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V-Measure\n",
    "\n",
    "Suppose we had the \"ground-truth\", i.e., true clustering labels for our data.  Then, we can measure the \"homogeneity\" criterion, and the completeness criterion.\n",
    "* A clustering result satisfies **homogeneity** if all of its clusters contain only samples which are members of the same ground-truth class.\n",
    "* A clustering result satisfies **completeness** if all the samples that are members of a ground-truth cluster are samples of the same cluster generated by the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# format: homogeneity_score(ground_truth, clustering)\n",
    "print \"homogeneity score = %g\\n\" % metrics.homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
    "print \"completeness score = %g\\n\" % metrics.completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"homogeneity score = %g\\n\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2])\n",
    "print \"completeness score = %g\\n\" % metrics.completeness_score([0, 0, 1, 1], [0, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"homogeneity score = %g\\n\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3])\n",
    "print \"completeness score = %g\\n\" % metrics.completeness_score([0, 0, 1, 1], [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"homogeneity score = %g\\n\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1])\n",
    "print \"completeness score = %g\\n\" % metrics.completeness_score([0, 0, 1, 1], [0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"homogeneity score = %g\\n\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0])\n",
    "print \"completeness score = %g\\n\" % metrics.completeness_score([0, 0, 1, 1], [0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *V-measure* is the harmonic mean between homogeneity and completeness:\n",
    "\n",
    "  $v = 2 * (homogeneity * completeness) / (homogeneity + completeness)$\n",
    "\n",
    "Questions:\n",
    "* will a permutation of the class or cluster label values change the score value in any way?\n",
    "* Is the metric symmetric wrt the switching label_true with label_pred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of using the V-measure**\n",
    "\n",
    "* Bounded scores: 0.0 is bad clustering, 1.0 is a perfect score.\n",
    "\n",
    "* Intuitive interpretation: clustering with bad V-measure can be qualitatively analyzed in terms of homogeneity and completeness to better feel what ‘kind’ of mistakes is done by the assignment.\n",
    "\n",
    "* No assumption is made on the cluster structure\n",
    "\n",
    "**Drawbacks**\n",
    "\n",
    "* The V-measure is not normalized with regards to random labeling: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence V-measure. In particular random labeling won’t yield zero scores especially when the number of clusters is large.\n",
    "\n",
    "+ This problem can safely be ignored for a large number of samples\n",
    "\n",
    "+ These metrics require knowledge of the ground truth classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Coefficient\n",
    "\n",
    "If the ground-truth cluster is not available, we can compute the silhouette coefficient. This Silhouette Coefficient is calculated using \n",
    "* $a$, the mean intra-cluster distance between elements in a cluster. The smaller the value, the better the cluster.\n",
    "* $b$, the mean nearest cluster distance for each sample.\n",
    "\n",
    "The Silhouette Coefficient for a sample is:\n",
    "\\begin{align}\n",
    "\\frac{b-a}{\\max{(a,b)}} \n",
    "\\end{align}\n",
    "\n",
    "Question:\n",
    "* what is a good silhouette value?\n",
    "* what is a poor silhouette value?\n",
    "* what does a silhouette value of 0 imply?\n",
    "\n",
    "We can use a silhouette value to estimate how many clusters to use for $k$-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Broadly speaking, there are two families of clustering techniques:\n",
    "* Partition algorithms: Start with a random partition and refine it iteratively.\n",
    "* Hierarchical algorithms -- Agglomerative (bottom-up) or top-down approaches\n",
    "\n",
    "Lets first look at partition algorithms.  Partitional algorithms can be divided in two groups:\n",
    "* Hard partition algorithms, such as *K-means*, assign a unique cluster value to each element in the feature space.\n",
    "* Soft partition algorithms, such as *Mixture of Gaussians*, can be viewed as density estimators and assign a confidence or probability to each point in the space.\n",
    "\n",
    "In order to build our intuition about clustering, we will start with the $K$-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  K-means algorithm\n",
    "\n",
    "1. Initialize the number, $K$, of desirable clusters.\n",
    "2. Split samples randomly into $K$ clusters.\n",
    "3. Compute the $K$ cluster centers\n",
    "4. Decide the class memberships of the $N$ data samples by assigning them to the nearest cluster centroids (e.g. the center of gravity or mean).\n",
    "5. If none of the N objects changed membership in the last iteration, exit. Otherwise return to step \\#3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#Create some data\n",
    "MAXN=40\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2), 5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,3]+1.2*np.random.randn(MAXN,2)])\n",
    "\n",
    "#Just for visualization purposes, create the labels of the 3 distributions\n",
    "y = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,3*np.ones((MAXN,1))])\n",
    "\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n",
    "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n",
    "_ = plt.title('Generated Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply scikit-learn toolkit to cluster the data into 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "K=3 # Assuming to be 3 clusters!\n",
    "\n",
    "clf = cluster.KMeans(init='random', n_clusters=K)\n",
    "clf.fit(X)\n",
    "\n",
    "# instead of assigning our data to clusters, we'll plot the \n",
    "# phase space: lay a grid, and predict which cluster each grid\n",
    "# node belongs to. \n",
    "\n",
    "x = np.linspace(-5,15,200)\n",
    "XX,YY = np.meshgrid(x,x)\n",
    "sz=XX.shape\n",
    "data=np.c_[XX.ravel(),YY.ravel()] \n",
    "\n",
    "Z=clf.predict(data)\n",
    "plt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower', \n",
    "           extent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\n",
    "plt.title('Clustering partition using k-means', size=14)\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n",
    "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n",
    "\n",
    "fig = plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Since we have the true labels for this toy problem, compute the completeness score, the homogeneity score, and the v-measure score, summarizing your observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some math behind the $K$-means algorithm...  the $K$-means algorithm seeks centroids that minimize the \"inertia\", a.k.a., the within-cluster sum-of-squares:\n",
    "\\begin{align}\n",
    "  \\text{Inertia}=\\displaystyle\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)|\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some quirks:\n",
    "* Inertia makes the assumption that clusters are convex and isotropic (WHY?). Hence, $K$-means performs terribly on elongated clusters, or manifolds with irregular shapes.\n",
    "\n",
    "* Convergence to global minimum? Given enough time, K-means will always converge, but to a local minimum.\n",
    "\n",
    "* The $k$-means algorithm requires the number of clusters to be specified a-priori. \n",
    "\n",
    "* The $k$-means algorithm scales well to a large number of samples, and has been used across a large range of application areas in many different fields.\n",
    "\n",
    "\n",
    "In fact, the computation is often done several times, with different random initial partitioning of the samples.  One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the init='kmeans++' parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "clf = cluster.KMeans(n_clusters=K, init='k-means++',  random_state=0,max_iter=300, n_init=10) \n",
    "clf.fit(X) \n",
    "\n",
    "print 'Final evaluation of the clustering:'\n",
    "print('Inertia: %.2f' %  clf.inertia_ )\n",
    "print('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),clf.labels_))\n",
    "print('Homogeneity %.2f' %  metrics.homogeneity_score(y.ravel(),clf.labels_))\n",
    "print('Completeness %.2f' %  metrics.completeness_score(y.ravel(),clf.labels_))\n",
    "print('V_measure %.2f' %  metrics.v_measure_score(y.ravel(), clf.labels_))\n",
    "print('Silhouette %.2f' %  metrics.silhouette_score(X, clf.labels_,metric='euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets run the k-means algorithm on a bunch of diferent generated data, a la\n",
    "# http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from itertools import cycle, islice\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(3, 12.5))\n",
    "\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    two_means = cluster.KMeans(n_clusters=params['n_clusters'])\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('KMeans', two_means),\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral clustering\n",
    "\n",
    "Spectral clustering has seen a lot of research activity and interest recently; many of the algorithms has been applied in image analysis and document clustering.  The idea is to use only the top $k$ eigenvalues of a matrix that gives some indication of similarity (or distance) between the observations.  The general algorithm proceeds as follows.\n",
    "Suppose $k$ is the number of clusters we seek.\n",
    "\n",
    "1. Construct a similarity matrix, $S$.  Recall, we have previously defined this similarity matrix as having the entries\n",
    "\\begin{align}\n",
    "  s_{ij} = s(\\vec{x}_i,\\vec{x}_j) = \\exp \\left(-\\gamma\\,d(\\vec{x}_i,\\vec{x}_j) \\right),\n",
    "\\end{align}\n",
    "where $d$ is the distance metric we have defined above.  The parameter $\\gamma$ determines how much affinity the two observations have.  There are some methods to choose the scaling parameter automatically. (See https://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf)\n",
    "\n",
    "2. Define a diagonal matrix whose elements, $d_{ii}$, is the row sum of the $i$th row of the similarity matrix,\n",
    "\\begin{align}\n",
    "D_{ii} = \\sum_{j=1}^n A_{ij}\n",
    "\\end{align}\n",
    "\n",
    "3. Construct the normalized Laplacian: $L = D^{-\\frac12}S\\,D^{-\\frac12}$.  \n",
    "\n",
    "4. Find the $k$ largest eigenvalues,$\\lambda_1 \\ge   \\lambda_2 \\ge \\cdots \\lambda_k$, and unit eigenvectors of $L$, \n",
    "$\\vec{u}_1, \\vec{u}_2, \\ldots, \\vec{u}_k$.  If there are repeated eigenvalues, choose eigenvectors that are orthogonal to each other.  For convenience, we will refer to eigenvectors as a matrix U, whose columns contain the eigenvectors,\n",
    "\\begin{align}\n",
    "U = [\\vec{u}_1, \\vec{u}_2, \\cdots, \\vec{u}_k]\n",
    "\\end{align}\n",
    "\n",
    "5. Form a new matrix, $Y$, by normalizing each **row** of U to have unit length,\n",
    "\\begin{align}\n",
    "  Y_{ij} = \\frac{U_{ij}}{ \\sqrt{\\sum_j U_{ij}^2} }\n",
    "\\end{align}\n",
    "\n",
    "6. Treating each row of $Y$ as a point in $R^k$, cluster them into $k$ clusters using the $K$-means algorithm, or any other suitable clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: it turns out, if you don't know a-priori how many clusters you want, you can look at the eigendecomposition of the graph Laplacian.  A large gap in the eigenvalues (difference between magnitudes of eigenvalues) is a strong indication of how many clusters you will need (https://www.sciencedirect.com/science/article/pii/S1063520315001414). \n",
    "(in the math world, we talk about \"intrinsic dimension\")\n",
    "\n",
    "Lets look at a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_moons\n",
    "\n",
    "[Xmoons, ymoons] = make_moons(n_samples=300, noise=.05)\n",
    "plt.scatter(Xmoons[:, 0], Xmoons[:, 1], c='r', marker='o',s=20) \n",
    "plt.axis('equal')\n",
    "plt.title('Connectivity-base clusters', size=14)\n",
    "fig = plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply spectral clustering to the two moons problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import euclidean_distances\n",
    " \n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)\n",
    " \n",
    "# normalize dataset for easier parameter selection\n",
    "X = StandardScaler().fit_transform(Xmoons)\n",
    " \n",
    "# Compute distances\n",
    "distances = euclidean_distances(Xmoons)\n",
    "    \n",
    "spectral = cluster.SpectralClustering(n_clusters=2, \n",
    "                                      affinity=\"nearest_neighbors\")\n",
    " \n",
    "spectral.fit(Xmoons)\n",
    "y_pred = spectral.labels_.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xmoons[y_pred==0, 0], Xmoons[y_pred==0, 1], c='b', \n",
    "            marker='o',s=20) \n",
    "plt.scatter(Xmoons[y_pred==1, 0], Xmoons[y_pred==1, 1], c='y', \n",
    "            marker='o',s=20) \n",
    "plt.axis('equal')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches((11,6))\n",
    "_ = plt.scatter(Xmoons[:, 0], Xmoons[:, 1], color=colors[y_pred].tolist(), \n",
    "            s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "Hierarchical clustering builds nested clusters by *merging or splitting* them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the *leaves* being the clusters with only one sample. This approach is popular because of its interpretability - the algorithm returns a *tree*, which shows similarity among the samples. *Partitioning* is computed by selecting a cut on that tree at a certain level. \n",
    "\n",
    "Two different approaches:\n",
    "1. Bottom-Up agglomerative clustering\n",
    "\n",
    "    * Starts with each sample data in a separate cluster.\n",
    "    \n",
    "    * Then, repeatedly joins the closest pair of clusters.\n",
    "    \n",
    "    * Stop when there is only one cluster.\n",
    "    \n",
    "    * history of merging forms a binary tree or hierarchy.\n",
    "\n",
    "2. Top-Down divisive clustering\n",
    "\n",
    "    * Start with all the data in a single cluster.\n",
    "    \n",
    "    * Consider every possible way to divide the cluster into two. Choose the best division.\n",
    "    \n",
    "    * Recursively operate on both sides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the agglomerative clustering (bottom-up) approach.  The natural question is: how do we measure the similarity of two clusters?  Book gives three different criterion, resulting in three different metrics for cluster merging:\n",
    "1. Maximum (complete) linkage minimize distance between observations of pairs of clusters.\n",
    "2. Average linkage averages similarity between members (minimizes the average of the distances between all observations of pairs of clusters)\n",
    "3. Ward linkage minimizes the sum of squared distances within all clusters.  (variance-minimizing approach)\n",
    "\n",
    "lets see how these three linkages work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXN1 =500\n",
    "MAXN2 =400\n",
    "MAXN3 =300\n",
    "X1 = np.concatenate ([2.25*np.random.randn(MAXN1,2),4+1.7* np.random.randn (MAXN2 ,2)])\n",
    "X1 = np.concatenate ([X1,[8,3]+1.9* np.random.randn(MAXN3 ,2)])\n",
    "y1 = np.concatenate ([ np.ones ((MAXN1,1)),2* np.ones((MAXN2,1))])\n",
    "y1 = np.concatenate ([y1,3* np.ones((MAXN3,1))]).ravel()\n",
    "y1 = np.int_(y1)\n",
    "labels_y1=['+','*','o']\n",
    "colors=['r','g', 'b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply agglomerative clustering using the different linkages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "for linkage in ('ward', 'complete', 'average'):\n",
    "    clustering = AgglomerativeClustering(linkage=linkage,n_clusters =3)\n",
    "    clustering.fit(X1)\n",
    "    \n",
    "    x_min , x_max = np.min (X1, axis =0) , np.max (X1,axis =0)\n",
    "    X1 = (X1 - x_min ) / ( x_max - x_min )\n",
    "    fig = plt.figure ()\n",
    "    fig.set_size_inches((5,5))\n",
    "    for i in range (X1.shape [0]) :\n",
    "        plt.text(X1[i,0],X1[i,1],labels_y1[y1[i]-1],color=colors[y1[i]-1])\n",
    "    plt.title (\"%s linkage\" % linkage,size =20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
