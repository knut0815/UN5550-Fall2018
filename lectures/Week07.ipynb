{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression (Chapter 7)\n",
    "\n",
    "This module focuses on a particular class of supervised machine learning: regression, where we are trying to discover relationships between variables in order to predict continuous quantities. Neelima gave you an introduction to linear regression, logistic regression, and how to use scikit-learn to accomplish both tasks. Content for this module:\n",
    "* linear least-squares \n",
    "* non-linear regression models\n",
    "* penalized regression models\n",
    "* constrained regression\n",
    "\n",
    "Resources:\n",
    "\n",
    "1. Gander, Gander and Kwok (2014), Scientific Computing: An introduction using Maple and MATLAB, Springer, https://www.springer.com/us/book/9783319043241\n",
    "2. Kuhn and Johnson (2013), Applied Predictive Modeling, Springer\n",
    "https://www.springer.com/us/book/9781461468486"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "For samples with $D$ features, linear regression attempts to fit a $D$-dimensional hyperplane to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "n = 200\n",
    "x = np.random.rand(n,1) # feature 1\n",
    "y = np.random.rand(n,1) # feature 2\n",
    "z = x + 0.5*y + 0.1*np.random.randn(n,1) + 0.2 # response variable, z(x,y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x,y,z,c='b',marker='o')\n",
    "ax.set_xlabel('feature 1')\n",
    "ax.set_ylabel('feature 2')\n",
    "_ = ax.set_zlabel('response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, we are trying to find coefficients $a_0 + a_1, a_2, \\ldots, a_D$ so that the hyperplane\n",
    "\n",
    "\\begin{align}\n",
    " z(x_1,x_2,\\ldots,x_D) = a_0 + a_1 x_1 + a_2 x_2 + \\cdots + a_D x_D\n",
    "\\end{align}\n",
    "\n",
    "models the response variable. (note, this is the generalize form of equation (6.2) in textbook).  Suppose we have $N$ samples, each with $D$ features, $\\vec{x}_i = [{x_i}_1,{x_i}_2,\\ldots,{x_i}_D],\\, i= 1,\\ldots N$. Then the hope, is that \n",
    "\n",
    "\\begin{align}\n",
    "  z({x_i}_1,{x_i}_2,\\ldots,{x_i}_D) \\approx y_i,\\quad \\text{for } i = 1,\\ldots, N\n",
    "\\end{align}\n",
    "\n",
    "This gives rise to an over-determined system of equations:\n",
    "\n",
    "\\begin{align}\n",
    "  a_0 + a_1 x_{11} + a_2 x_{12} + \\cdots + a_D x_{1D} &\\approx y_1\\\\\n",
    "  a_0 + a_1 x_{21} + a_2 x_{22} + \\cdots + a_D x_{2D} &\\approx y_2\\\\\n",
    "    &\\vdots \\\\\n",
    "  a_0 + a_1 x_{N1} + a_2 x_{N2} + \\cdots + a_D x_{ND} &\\approx y_N\\\\\n",
    "\\end{align}\n",
    "\n",
    "This is more easily expressed in matrix form, $X\\,a = y$,\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1D}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2D}\\\\\n",
    "1 & x_{31} & x_{32} & \\cdots & x_{3D}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_{N1} & X_{x2} & \\cdots & x_{ND}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a_0\\\\\n",
    "a_1 \\\\\n",
    "\\vdots\\\\\n",
    "a_D\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Here, the matrix $X$ is of size: $N \\times (D+1)$, the vector of coefficients we seek is of size $(D+1)\\times 1$, the right-hand side vector is of size $N \\times 1$.  Here, we are imagining that $N \\gg D$.  How do we solve the overdetermined system?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it turns out we can't exactly most of the time, i.e., we can not find $a$ such that $X\\,a =y$.  There is always some mismatch, a residual: $r = X\\,a - y$.  We want to minimize this residual, for example, minimize $\\|r\\|^2_2 = \\|X\\,a - y\\|^2_2$, i.e.,\n",
    "\n",
    "$$ \\min \\|X\\,a - y \\|^2_2 = \\min \\left( \\sum_{i = 1}^N  (X\\,a - y)_i^2 \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem\n",
    "\n",
    "If $X^T (X\\,a - y) = 0$, then $a$ solves the linear least squares problem, i.e. $a$ minimizes $\\|X\\,a - y\\|_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: Let $c$ be any vector of size $(D+1)\\times 1$.  Then $X(a+c) - y = X\\,a -y + X\\,c $, and\n",
    "\n",
    "\\begin{align}\n",
    "\\|X(a+c) - y\\|_2^2 &= \\left( X\\,a -y + X\\,c \\right)^T \\left(X\\,a -y + X\\,c \\right) \\\\\n",
    "&= (X\\,a - y)^T (X\\,a -y) + 2 (X\\,c)^T(X\\,a - y) + (X\\,c)^T (X\\,c) \\\\\n",
    "&= \\|X\\,a - y\\|_2^2 + 2c^T X^T (X\\,a - y) + \\|X\\,c\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "If $X^T (X\\,a - y) = 0$, then\n",
    "\\begin{align}\n",
    "\\|X(a+c) - y\\|_2^2 = \\|X\\,a - y\\|_2^2 + \\|X\\,c\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "Hence, for every $c$, we have \n",
    "\\begin{align}\n",
    "\\|X(a+c) - y\\|_2^2 \\ge  \\|X\\,a - y\\|_2^2\n",
    "\\end{align}\n",
    "\n",
    "so $a$ must minimize $\\|X\\,a - y\\|_2^2$ as required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation $X^T(X\\,a - y) = 0$ is often written as\n",
    "\n",
    "$$ X^T X\\,a = X^T y, $$\n",
    "\n",
    "called the normal equations.  There is a geometric interpretation: the vector in the range (column space of $X$) that lies closest to $y$ makes $X\\,a - y$ perpendicular to the range.  Note, \n",
    "* the normal equations expresses the $N\\times(D+1)$ linear least squares problem as a $(D+1)\\times (D+1)$ linear system.\n",
    "* the matrix $X^T X$ is symmetric\n",
    "* $X^T X$ is singular . if and only if the columns of $X$ are linearly dependent, i.e., the rank of $X$ is less than $(D+1)$\n",
    "* if $X^T X$ is non singular, then it is positive definite.\n",
    "\n",
    "Observe that if $X^T X$ is non singular, then\n",
    "$$ X^T X\\,a = X^T y, $$\n",
    "can be written as \n",
    "$$ a = (X^T X)^{-1} X^T y, $$\n",
    "\n",
    "One sometimes defines the pseudo inverse,\n",
    "$$ X^\\dagger = (X^T X)^{-1} X^T.$$\n",
    "Then, the least squares problem $X\\,a = y$ has a solution $a = X^\\dagger y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerically, one never forms the pseudo inverse to solve the least squares problem.  Rather, we rely on a useful factorization known as the QR factorization.  Every matrix has a QR factorization, $X = QR$, where $Q$ is an orthogonal matrix, and $R$ is upper triangular.  For convenience, lets refer to\n",
    "* $X$ as an $m \\times n$ matrix\n",
    "* $Q$ is an $m \\times m$ matrix, whose columns are orthonormal to each other, i.e.:\n",
    "    * $q_i \\cdot q_j = 0$ if $i\\neq j$\n",
    "    * $q_i \\cdot q_i = 1, \\quad i=1,2,\\ldots,m$\n",
    "* $R$ is an $m \\times n$ upper triangular matrix, i.e., $r_{ij} = 0$ if $i > j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets switch to a toy example where we have an over-determined system, and explore the QR factorization and least-squares solution.  Suppose we have two packages that we wish to ship.  We measure the weight of these two packages in the office.  Package $A$ measures in at 2 pounds, package $B$ measures in at 5 pounds.  At the distribution site however, the two packages are weighed together, and the joint weight is reported at 8 pounds.  Use a least-squares solution to find a best estimate for the weight of each package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our matrix X:\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array( [ [1,0], [0,1], [1,1]])\n",
    "print(\"our matrix X:\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets check that Q*R = X, up to machine precision\n",
      "[[ 1.00000000e+00 -2.22044605e-16]\n",
      " [ 0.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "[Q,R] = np.linalg.qr(X, mode = 'complete')\n",
    "print(\"Lets check that Q*R = X, up to machine precision\")\n",
    "print(np.matmul(Q,R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that $Q$ has orthonormal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix Q: \n",
      "[[-0.70710678  0.40824829 -0.57735027]\n",
      " [-0.         -0.81649658 -0.57735027]\n",
      " [-0.70710678 -0.40824829  0.57735027]]\n",
      "\n",
      "Various inner products:\n",
      "q1^t q_1 = 1\n",
      " \n",
      "q1^t q_2 = 8.6508e-17 \n",
      "\n",
      "q1^t q_3 = -1.05825e-16 \n",
      "\n",
      "q2^t q_2 = 1 \n",
      "\n",
      "q2^t q_3 = 0 \n",
      "\n",
      "q3^t q_3 = 1 \n"
     ]
    }
   ],
   "source": [
    "print(\"matrix Q: \")\n",
    "print(Q)\n",
    "print(\"\\nVarious inner products:\")\n",
    "print(\"q1^t q_1 = %g\\n \" % np.dot(Q[:,0],Q[:,0]) )\n",
    "print(\"q1^t q_2 = %g \\n\" % np.dot(Q[:,0],Q[:,1]) )\n",
    "print(\"q1^t q_3 = %g \\n\" % np.dot(Q[:,0],Q[:,2]) )\n",
    "print(\"q2^t q_2 = %g \\n\" % np.dot(Q[:,1],Q[:,1]) )\n",
    "print(\"q2^t q_3 = %g \\n\" % np.dot(Q[:,1],Q[:,2]) )\n",
    "print(\"q3^t q_3 = %g \" % np.dot(Q[:,2],Q[:,2]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that $R$ is upper triangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = \n",
      "[[-1.41421356 -0.70710678]\n",
      " [ 0.         -1.22474487]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"R = \")\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a reduced factorization that is more useful. $ X = \\hat{Q} \\hat{R}$, where $\\hat{Q}$ is an $m\\times n$ matrix with orthonormal columns, and $\\hat{R}$ is an $n\\times n$ upper triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q = \n",
      "[[-0.70710678  0.40824829]\n",
      " [-0.         -0.81649658]\n",
      " [-0.70710678 -0.40824829]]\n",
      "R = \n",
      "[[-1.41421356 -0.70710678]\n",
      " [ 0.         -1.22474487]]\n",
      "Lets check that Q*R = X, up to machine precision\n",
      "[[ 1.00000000e+00 -2.22044605e-16]\n",
      " [ 0.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "[Q,R] = np.linalg.qr(A, mode = 'reduced')\n",
    "print(\"Q = \")\n",
    "print(Q)\n",
    "print(\"R = \")\n",
    "print(R)\n",
    "print(\"Lets check that Q*R = X, up to machine precision\")\n",
    "print(np.matmul(Q,R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we use the $QR$ factorization to solve the normal equations?  Suppose we have $X = \\hat{Q}\\,\\hat{R}$.  Observe:\n",
    "\\begin{align}\n",
    "X^T X a &= X^T y \\\\\n",
    "(\\hat{Q}\\,\\hat{R})^T (\\hat{Q}\\,\\hat{R})\\, a &= (\\hat{Q}\\,\\hat{R})^T y \\\\\n",
    "\\hat{R}^T \\hat{Q}^T \\hat{Q}\\, \\hat{R} \\, a &= \\hat{R}^T \\hat{Q}^T y\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\hat{R}^T$ exists, and observing that $\\hat{Q}^T \\hat{Q}$ gives an $n\\times n$ identity matrix,\n",
    "\\begin{align}\n",
    "&\\implies \\hat{R}^T \\hat{R} \\, a = \\hat{R}^T \\hat{Q}^T y \\\\\n",
    "&\\implies  \\hat{R} \\, a = \\hat{Q}^T y\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use our toy example to fit a regression line, first using sci-kit learn, and then using our derived normal equations, simplified using the QR factorization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
