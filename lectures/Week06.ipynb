{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning - Part II (Chapter 5)\n",
    "\n",
    "This module focuses on a particular class of supervised machine learning: classification, where we have a finite number of choices to label an observation. \n",
    "\n",
    "Topics for this module:\n",
    "* Cross-Validation\n",
    "* Learning Curves\n",
    "* Support Vector Machines\n",
    "* Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended the last workbook by comparing various classification methods built into the Scikit-Learn toolbox, using the iris dataset.   Lets reproduce those plots using the wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets # import standard datasets\n",
    "wine = datasets.load_wine() # load wine data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as the KNN classifier, decision tree, and support vector classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generated boxplots of accuracy by splitting, fitting and predicting training/test data respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1 = neighbors.KNeighborsClassifier(n_neighbors = 1)\n",
    "nn3 = neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "nn5 = neighbors.KNeighborsClassifier(n_neighbors = 5)\n",
    "svc = svm.SVC()\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "acc = np.zeros((20,5))\n",
    "\n",
    "for i in xrange(20):\n",
    "    # split the data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.4)\n",
    "\n",
    "    # train the classifier using the training data\n",
    "    nn1.fit(x_train, y_train)\n",
    "    nn3.fit(x_train, y_train)\n",
    "    nn5.fit(x_train, y_train)\n",
    "    svc.fit(x_train, y_train)\n",
    "    dt.fit(x_train, y_train)\n",
    "\n",
    "    # compute the prediction of the test set using the model\n",
    "    yhat_nn1 = nn1.predict(x_test)\n",
    "    yhat_nn3 = nn3.predict(x_test)\n",
    "    yhat_nn5 = nn5.predict(x_test)\n",
    "    yhat_svc = svc.predict(x_test)\n",
    "    yhat_dt = dt.predict(x_test)\n",
    "\n",
    "    acc[i][0] = accuracy_score(yhat_nn1,y_test)\n",
    "    acc[i][1] = accuracy_score(yhat_nn3,y_test)\n",
    "    acc[i][2] = accuracy_score(yhat_nn5,y_test)\n",
    "    acc[i][3] = accuracy_score(yhat_svc,y_test)\n",
    "    acc[i][4] = accuracy_score(yhat_dt,y_test)\n",
    "    \n",
    "    \n",
    "# generate box plot\n",
    "plt.boxplot(acc);\n",
    "for i in xrange(5):\n",
    "    # add jigger to plot data\n",
    "    xderiv = (i+1)*np.ones(acc[:,i].shape)+(np.random.rand(20,)-0.5)*0.1\n",
    "    plt.plot(xderiv,acc[:,i],'bo',alpha=0.3)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(['1-NN','3-NN','5-NN','SVM','Decission Tree'])\n",
    "_ = plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree appears to perform the best, based on our simple cross-validation test.  Here, we ran several experiments where we randomly split the data set into a training and a test set, did the prediction, and fit.  This is known as repeated random sub-sampling validation (or Monte Carlo cross-validation).  There are other modes of cross validation:\n",
    "* *leave-one-out*:  Given $N$ samples, model is trained with $N-1$ samples and tested with the remaining one.  This is repeated $N$ times, once per training sample, and the result is averaged\n",
    "* *leave-p-out*:  Given $N$ samples, model is trained with $N-p$ samples and tested with the remaining $p$ samples.  This is repeated $N \\choose p$ times, and the result is averaged.  This approach is impractical for most choices of $n$ and $p$.\n",
    "* *k-fold cross-validation*: the data is split into $k$ non-overlapping splits.  Use $k-1$ splits for training, and the remaining split for testing.  Repeat $k$ times, leaving one split out each time, then average the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Lets learn about decision trees (DT), before switching to errors associated with the learning task.  DTs are a popular method for various machine learning tasks because \n",
    "* they are invariant under scaling and various other transformation of feature values (i.e. normalization not needed)\n",
    "* one can include irrelevant features without affecting the final outcome\n",
    "* can handle both numerical and categorical data \n",
    "* the final outcome is easy to understand / interpret / visualized. \n",
    "\n",
    "DT have various downsides however:\n",
    "* prone to over-fitting (overly-complex trees)\n",
    "* unstable: small variations in data might result in a completely different generated tree\n",
    "* optimal decision trees are NP-complete; hence, most algorithms are based on heuristic  algorithms\n",
    "\n",
    "What does a tree look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(max_depth=2)\n",
    "x_train, x_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3)\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "                         feature_names=wine.feature_names,  \n",
    "                         class_names=wine.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read this tree? \n",
    "* top entry in each box gives the condition being tested.  \n",
    "* gini: is a measure of impurity; how often an randomly chosen element from the set would be incorrectly labeled, \n",
    "* samples: number of samples in each leaf\n",
    "* value: (???) presumably, some values in the data?  I'm not sure.\n",
    "* class: the targeted output\n",
    "\n",
    "## How is a tree built?\n",
    "\n",
    "The general approach is to split a set of samples into subsets based on some attributes, and repeating the process in a recursive manner.  The simplest algorithm is a top-down, greedy search through the space of possible decision trees.\n",
    "First, one computes the *entropy* for the data set, a measure of uncertainty in the data set, \n",
    "$$ H(S) = \\sum_{c \\in C} -p(c) \\log_2{p(c)}.$$\n",
    "Here,\n",
    "* $S$ is the current sample set for which entropy is being calculated\n",
    "* $C$ is the set of classes in $S$\n",
    "* $p(c)$ is the probability of encountering element in class $c$ in the set $S$.\n",
    "\n",
    "For a binary classification problem:\n",
    "* an entropy will be zero if all samples are either true, or if all samples are false.\n",
    "* if half the samples are true, half the samples are false, then the entropy will be one (i.e. high).\n",
    "\n",
    "The greedy algorithm proceeds as follows:\n",
    "* compute entropy of parent\n",
    "* for each feature/attribute\n",
    "    * compute information gain:  Entropy(parent) - Weighted Sum of Entropy(Children).\n",
    "* pick feature that gives largest information gain.\n",
    "\n",
    "This is then repeated recursively for each leaf in the tree.  Wikipedia has a nice example for a toy data set with four attributes / 14 data points.  https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain\n",
    "\n",
    "Alternatively, Scikit-learn uses a hierarchical approach known as CART (Classification and Regression Trees). The main difference, is the Gini Index is used as the cost function to evaluate splits in the data set.  It is beyond the scope of this course to explore CART and it's variants.  Interested readers should look at the collection: \"The Top Ten Algorithms in Data Mining\", edited by Wu and Kumar. A PDF of the relevant chapter appears to presently be posted at: http://www.uta.edu/faculty/rcli/Teaching/math6310/materials/Ten.pdf#page=192\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves\n",
    "\n",
    "Denote the training error (i.e. in-sample error) as $E_{in}$, i.e., the error in the model measured over all data in the *training* set.   Denote the testing error (i.e. out-of sample error / generalization error) as $E_{out}$, i.e., the error expected on unseen data.  Some intuitive statements:\n",
    "* $E_{out} \\ge E_{in}$\n",
    "* want $E_{in} \\to 0$\n",
    "* want $E_{out} \\approx E_{n}$, i.e.,\n",
    "$\\quad  E_{in} \\le E_{out} \\le E_{in} + \\Omega, \\quad \\text{with } \\Omega \\to 0,$ where $\\Omega$ typically depends on the number of samples $N$, complexity of the model, ...\n",
    "\n",
    "Learning curve: shows relationship between training/test errors as a function of ML problem parameters.  We'll use the Decision Tree classifier which seems to work best for this problem.  Additionally, we will control the maximum depth of the tree, in some sense, controlling the complexity of the ML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "Ein = np.zeros((99,40))\n",
    "Eout = np.zeros((99,40))\n",
    "\n",
    "for nratio in xrange(99):\n",
    "\n",
    "    ratio = 1 - (nratio+1)/100.0\n",
    "    for i in xrange(40):\n",
    "        # split the data\n",
    "        x_train, x_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=ratio)\n",
    "\n",
    "        # train the classifier using the training data\n",
    "        dt.fit(x_train, y_train)\n",
    "\n",
    "        # compute the prediction of the test set using the model\n",
    "        yhat_train = dt.predict(x_train)\n",
    "        yhat_test = dt.predict(x_test)\n",
    "\n",
    "        Ein[nratio][i] = 1 - accuracy_score(yhat_train,y_train)\n",
    "        Eout[nratio][i] = 1 - accuracy_score(yhat_test,y_test)\n",
    "    \n",
    "p1,=plt.plot(np.mean(Ein[:,:].T,axis=0),'pink')\n",
    "p2,=plt.plot(np.mean(Eout[:,:].T,axis=0),'c')\n",
    "fig = plt.gcf()\n",
    "#fig.set_size_inches(12,5)\n",
    "plt.xlabel('Percent of samples used for training')\n",
    "plt.ylabel('Error rate')\n",
    "_ = plt.legend([p1,p2],[\"Training Error, Depth = 1\",\"Test Error, Depth = 1\"])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* ? \n",
    "* ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets repeat with a more complicated model (increasing depth of tree permitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "# precomputed from above execution block\n",
    "p1,=plt.plot(np.mean(Ein[:,:].T,axis=0),'pink')\n",
    "p2,=plt.plot(np.mean(Eout[:,:].T,axis=0),'c')\n",
    "\n",
    "Ein = np.zeros((99,40))\n",
    "Eout = np.zeros((99,40))\n",
    "\n",
    "for nratio in xrange(99):\n",
    "\n",
    "    ratio = 1 - (nratio+1)/100.0\n",
    "    for i in xrange(40):\n",
    "        # split the data\n",
    "        x_train, x_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=ratio)\n",
    "\n",
    "        # train the classifier using the training data\n",
    "        dt.fit(x_train, y_train)\n",
    "\n",
    "        # compute the prediction of the test set using the model\n",
    "        yhat_train = dt.predict(x_train)\n",
    "        yhat_test = dt.predict(x_test)\n",
    "\n",
    "        Ein[nratio][i] = 1 - accuracy_score(yhat_train,y_train)\n",
    "        Eout[nratio][i] = 1 - accuracy_score(yhat_test,y_test)\n",
    "    \n",
    "p3,=plt.plot(np.mean(Ein[:,:].T,axis=0),'red')\n",
    "p4,=plt.plot(np.mean(Eout[:,:].T,axis=0),'blue')\n",
    "fig = plt.gcf()\n",
    "#fig.set_size_inches(12,5)\n",
    "plt.xlabel('Percent of samples used for training')\n",
    "plt.ylabel('Error rate')\n",
    "_ = plt.legend([p1,p2,p3,p4],[\"Training Error, Depth = 1\",\"Test Error, Depth = 1\",\"Training Error, Depth = 2\",\"Test Error, Depth = 2\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs are not as instructive as those in your textbook, since we are working with the wine data set, rather than the toy problem generated on page 79.  Some terms:\n",
    "* the training and test errors often converge for each ML model.  The value it converges to is known as the bias.  Here the curves have not converged, which is often the case when there are insufficient samples.\n",
    "* the difference between the bias and the test error is known as the variance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OverFitting\n",
    "\n",
    "If one creates a learning curve for a fixed number of data samples, but increase the complexity of the model, one often observes the following behavior.  (We'll use the data from the textbook since we don't really have enough samples to see such a curve in the wine data set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "MAXC=20\n",
    "N=1000\n",
    "NTEST=4000\n",
    "ITERS=3\n",
    "\n",
    "yhat_test=np.zeros((ITERS,MAXC,2))\n",
    "yhat_train=np.zeros((ITERS,MAXC,2))\n",
    "\n",
    "#Repeat ten times to get smooth curves\n",
    "for i in xrange(ITERS):\n",
    "    X = np.concatenate([1.25*np.random.randn(N,2),5+1.5*np.random.randn(N,2)]) \n",
    "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(N,2)])\n",
    "    y = np.concatenate([np.ones((N,1)),-np.ones((N,1))])\n",
    "    y = np.concatenate([y,np.ones((N,1))])\n",
    "    perm = np.random.permutation(y.size)\n",
    "    X = X[perm,:]\n",
    "    y = y[perm]\n",
    "\n",
    "    X_test = np.concatenate([1.25*np.random.randn(NTEST,2),5+1.5*np.random.randn(NTEST,2)]) \n",
    "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(NTEST,2)])\n",
    "    y_test = np.concatenate([np.ones((NTEST,1)),-np.ones((NTEST,1))])\n",
    "    y_test = np.concatenate([y_test,np.ones((NTEST,1))])\n",
    "\n",
    "    j=0\n",
    "    for C in xrange(1,MAXC+1):\n",
    "        #Evaluate the model\n",
    "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
    "        clf.fit(X,y.ravel())\n",
    "        yhat_test[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
    "        yhat_train[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X), y.ravel())\n",
    "        j=j+1\n",
    "\n",
    "p1, = plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),'r')\n",
    "p2, = plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'b')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.xlabel('Complexity')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend([p1, p2], [\"Testing error\", \"Training error\"])\n",
    "plt.savefig(\"learning_curve_4.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here, we see that as the complexity increases, the training error decreases.\n",
    "* Above a certain level of complexity, the test error starts increasing.  This is known as **over-fitting**\n",
    "\n",
    "Most models are parameterized by hyper-parameters.\n",
    "* e.g., nearest neighbors: have to specify number of neighbors to use\n",
    "* e.g., decision tree: have to specify depth\n",
    "\n",
    "A good heuristic for selecting the model is to choose the value of the hyper-parameter that yields the smallest estimated test error.  (test this using cross-validation).  To address over-fitting, the following approaches are also used:\n",
    "* regularization: penalizing complex models\n",
    "* ensemble techniques: e.g., bagging.  The idea is to generate several subsets of data from the training sample chosen randomly with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: general classification process\n",
    "\n",
    "Since we often need to select the best hyper-parameters for our model, we now need to divide the data into three sets: training, validation and test sets.  \n",
    "* training set: what we train the models on\n",
    "* validation set: selecting a model based on reducing the out-of-sample error\n",
    "* testing set: use exclusively for accessing performance.  (never used for learning)\n",
    "\n",
    "Practically, because we are now suggesting to split the data into three sets, the classifier is trained with a smaller fraction of the data.  Often, it is best to use a *nested* cross-validation.  We call it a nested cross-validation because we have an *inner* and *outer* cross-validation; the inner cross validation is what we discussed earlier, testing to select the best model from various algorithms. The outer cross validation applies cross-validation to find the best hyper-parameter.  \n",
    "\n",
    "Here is a five-fold cross validation of the wine data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "#Create a 10-fold cross validation set\n",
    "kf=cross_validation.KFold(n=wine.data.shape[0], n_folds=5, shuffle=True)\n",
    "      \n",
    "#S possible hyper-parameters to check for the decision tree\n",
    "C=np.arange(2,10,)\n",
    "\n",
    "# we'll run 20 \n",
    "acc = np.zeros((5,9))\n",
    "i=0\n",
    "for train_index, test_index in kf:\n",
    "    x_train, x_test = wine.data[train_index], wine.data[test_index]\n",
    "    y_train, y_test = wine.target[train_index], wine.target[test_index]\n",
    "    j=0\n",
    "    for c in C:\n",
    "        dt = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=c)\n",
    "        dt.fit(x_train,y_train)\n",
    "        yhat = dt.predict(x_test)\n",
    "        acc[i][j] = metrics.accuracy_score(yhat, y_test)\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "    \n",
    "plt.boxplot(acc);\n",
    "for i in xrange(4):\n",
    "    xderiv = (i+1)*np.ones(acc[:,i].shape)+(np.random.rand(5,)-0.5)*0.1\n",
    "    plt.plot(xderiv,acc[:,i],'ro',alpha=0.3)\n",
    "\n",
    "print 'Mean accuracy: ' + str(np.mean(acc,axis = 0))\n",
    "print 'Selected model index: ' + str(np.argmax(np.mean(acc,axis = 0)))\n",
    "print 'Complexity: ' + str(C[np.argmax(np.mean(acc,axis = 0))])\n",
    "plt.ylim((0.7,1.))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.xlabel('Complexity')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(\"model_selection.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Many practitioners believe that support vector machines (SVMs) are the best off-the-shelf supervised learning algorithms.  I have insufficient evidence/experience to confirm to refute this claim.  What I can say, is of the four researchers who utilize ML learning that I interact with, half use SVMs, half use random forest (RF).\n",
    "\n",
    "Section 5.7.2.1 gives one viewpoint on how to derive the optimization problem that is posed by SVM.  It was mathematically unsatisfying for me, and makes some implicit assumptions on the coefficients of the hyperplanes, $\\vec{a}$.  \n",
    "\n",
    "I was going to write up a set of notes, but found this nice presentation that was given at the AMIA 2009 meeting.  https://med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "The random forest (RF) is a decision tree type algorithm, where different trees are created by \"randomly\" hiding features available, and generating many trees, each constructed with different features available, and each with access to a random set of the training samples.  The idea, is that each tree brings a different background / view of the information to the problem.  Pooling decisions made by the collection of decision trees, will be more accurate, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
