{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning (Chapter 5)\n",
    "\n",
    "There are two broad classes of machine learning tasks:\n",
    "1. Supervised Learning: training set with desired outputs available\n",
    "2. Unsupervised Learning: no labels available; algorithm has to find structure on its own.\n",
    "\n",
    "This module focuses on a particular class of supervised machine learning: classification, where we have a finite number of choices to label an observation. \n",
    "\n",
    "Goals for this module:\n",
    "* Exposure to the Scikit-learn toolbox\n",
    "* Metrics to evaluate performance of ML algorithm\n",
    "* Training / Validation / Testing\n",
    "* Awareness of some ML algorithms and their mathematical foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are given $n$ samples of data which belong to two (or more) classes.   We want to utilize an algorithm to learn from these already labeled data in order to predict the class of unlabeled data.  Each data has at least one feature (a.k.a. attribute).  We sometimes refer to this data as *multi-variate* data.  The terminology used in the ML domain varies greatly.  \n",
    "* features / attributes / dimensions / regressors / covariates / predictors / independent variables are used interchangably.\n",
    "* samples / instances / examples \n",
    "* classes / label / outcome / response / dependent variable\n",
    "\n",
    "We will make use of sci-kit learn, https://en.wikipedia.org/wiki/Scikit-learn, which has various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data for Scikit-learn\n",
    "\n",
    "Sci-kit learn input data is structured in Numpy arrays, organized as $$X := n\\_samples \\times n\\_features$$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets # import standard datasets\n",
    "\n",
    "iris = datasets.load_iris() # load iris data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample datasets are dictionary objects that hold the data and some related metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual feature array is stored in the .data member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris.data.shape[1]\n",
    "samples = iris.data.shape[0]\n",
    "print \"number of samples = %g, number of features = %g\"%(samples,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Predicting\n",
    "\n",
    "All ML objects in Scikit-learn share a uniform API for learning and predicting, namely, fit() and predict().  We begin with the $k$ nearest neighbors (KNN) algorithm, which is a widely used technique because it can be evaluated very quickly, and it is easy to interpret the output. How does the KNN algorithm work?  For each test sample:\n",
    "1. Pick a value of $k$: the number of neighbors we wish to use for classification.\n",
    "2. Compute some measure between the test sample and each training sample.  Often, one uses the Euclidean distance, but other metrics might be appropriate.\n",
    "3. Sort the computed distances in increasing order based on distance values\n",
    "4. for the $k$ closest training samples, return the most frequent class as the predicted class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "# create an instance of K-nearest neighbor classifer\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "# train the classifier using the all the samples, except the last one\n",
    "knn.fit(iris.data[:-1], iris.target[:-1] )\n",
    "\n",
    "# compute the prediction of the last sample according to the model\n",
    "predict  = knn.predict(iris.data[-1:])\n",
    "\n",
    "print \"actual classification: \" + iris.target_names[iris.target[-1]]\n",
    "print \"predicted classification: \" + iris.target_names[predict[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to visualize the classification boundary for four-dimensional data (sepal length, sepal width, petal length, petal width).  We could take a slice of the four-dimensional space to visualize the classification along two dimensions.  Instead, suppose we only fit the training samples using the first two features, sepal length and sepal width (in cm).  We can visualize the classification boundary for this classification by laying down a mesh, and running a predictor at the mesh values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris.data[:,:2] # take only the first two columns\n",
    "y = iris.target\n",
    "\n",
    "# train classifier\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 11)\n",
    "knn.fit(x, y)\n",
    "\n",
    "# setup mesh\n",
    "h = 0.1\n",
    "x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "xg, yg = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# run classifier on mesh\n",
    "z = knn.predict(np.c_[xg.ravel(), yg.ravel()])\n",
    "\n",
    "# specifying some colors\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# plot classifier\n",
    "zg = z.reshape(xg.shape)\n",
    "_ = plt.figure()\n",
    "_ = plt.pcolormesh(xg,yg,zg,cmap=cmap_light)\n",
    "\n",
    "# overlay with training data points\n",
    "_ = plt.scatter(x[:,0],x[:,1],c=y, cmap=cmap_bold, edgecolor='k')\n",
    "\n",
    "# setting the aspect ratio so we can visualize distance properly\n",
    "_ = plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance measures\n",
    "\n",
    "The most basic way to measure performance of a classifier is its accuracy, $$ acc = \\frac{\\text{Number of correct predictions}}{n}.$$  Lets return to the full iris data set, train the model on all the data, and run the predictor on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 11)\n",
    "\n",
    "# train the classifier using the all the samples\n",
    "knn.fit(iris.data, iris.target)\n",
    "\n",
    "# compute the prediction each sample using the model\n",
    "predict  = knn.predict(iris.data)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predict,iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a surprisingly good model.  However, keep in mind that we have fit the model to the data, so we won't know how it will perform on unseen data.  We'll address this later.  First, one might need a more detailed analysis for the prediction of elements within classifications.  If we were doing a binary classification (e.g. true / false), we would normally present a confusion matrix, that compares the predictor to the ground truth\n",
    "<table border=1px>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th colspan=2 text-align=\"center\">Ground Truth </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Prediction </th>\n",
    "        <th>Positive</th>\n",
    "        <th>Negative</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> Positive </th>\n",
    "        <td> TP </td>\n",
    "        <td> FP </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> Negative </th>\n",
    "        <td> FN </td>\n",
    "        <td> TN </td>\n",
    "    </tr>\n",
    "</table>\n",
    "where \n",
    "\n",
    "* TP: true positives: classifier predicts a sample as positive in accordance with ground truth\n",
    "* FP: false positive: classifier predicts a sample as positive in conflict with ground truth\n",
    "* TN: true negative:  classifier predicts a sample as negative in accordance with ground truth\n",
    "* FN: false negative: classifier predicts a sample as negative in conflict with ground truth\n",
    "\n",
    "Note the similarity with the type-I and type-II error we saw previously in hypothesis testing. Using this notation, the accuracy is $$ \\text{accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}}.$$ There are a few additional metrics of interest:\n",
    "\n",
    "* sensitivity: \n",
    "$$ \\text{sensitivity} = \\frac{\\text{TP}}{\\text{TP + FN}}$$\n",
    "* specificity: $$ \\text{specificity} = \\frac{\\text{TN}}{\\text{TP + FN} }$$\n",
    "* precision: $$ \\text{precision} = \\frac{\\text{TP}}{\\text{TP + FP} }$$\n",
    "* Negative Predictive Value: $$ \\text{NPV} = \\frac{\\text{TN}}{\\text{TN + FN} }$$\n",
    "\n",
    "\n",
    "Question: what do we do when there are more classes? well, we can generate a $k \\times k$ matrix, where $k$ is the number of classes.  Four our iris example:\n",
    "<table border=1px>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th colspan=3 text-align=\"center\">Ground Truth </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Prediction </th>\n",
    "        <th>setosa</th>\n",
    "        <th>versicolor</th>\n",
    "        <th>virginica</th>       \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> setosa </th>\n",
    "        <td> TC </td>\n",
    "        <td> FC </td>\n",
    "        <td> FC </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> versicolor </th>\n",
    "        <td> FC </td>\n",
    "        <td> TC </td>\n",
    "        <td> FC </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> virginica </th>\n",
    "        <td> FC </td>\n",
    "        <td> FC </td>\n",
    "        <td> TC </td>\n",
    "    </tr>\n",
    "</table>\n",
    "where we now have\n",
    "\n",
    "* TC: true classification\n",
    "* FC: false classification, where you could even specify further by extending the idea of false negative or false positive, for example, falsely identified setosa as versicolor.\n",
    "\n",
    "Lets generate the confusion matrix for our classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.empty([3,3])\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        C[i,j] = np.sum(np.logical_and(predict==i, iris.target==j))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our classifier identifies all the setosa variety perfectly; it identified 48/50 versicolor correctly, and misclassified 2 versicolors as verginica; the classifier identified 48/50 virginica variety correctly, and misclassified 2 virginica varieties as versicolor.  In order to extend the performance metrics from the binary classification (i.e. sensitivity, specificity, precision or NPV), a pair-wise comparison is needed, i.e., setosa -- non-setosa, versicolor -- non-versicolor, virginica -- non-verginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the performance measures look pretty good, we have thus far checked the classifier performance on the same data that it has been trained on.  We have no way to measure how the model will behave on unlabeled data.  The common approach is to split the data into a **training** set and a **test** set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.33, random_state=10)\n",
    "# note: test_size is a number between 0 and 1,\n",
    "# random_state is the seed used by the random number generator.  Here, I've fixed it to 10 for reproducibility\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "# train the classifier using the training data\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# compute the prediction of the test set using the model\n",
    "predict  = knn.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print \"accuracy = %g \"%(accuracy_score(predict,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.empty([3,3])\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        C[i,j] = np.sum(np.logical_and(predict==i, y_test==j))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which allows us to conclude that the classifier had a bit of problem trying to classify the verginica species. \n",
    "\n",
    "Of course, each time we randomly split the dataset, we will get a difference measure of performance.  A good simulation would run this classification experiment many times and average the performance.  Instead of computing the confusion matrix, we will just measure the accuracy (number of correctly identified species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 11)\n",
    "\n",
    "accuracy = np.zeros((10,))\n",
    "\n",
    "for i in xrange(10):\n",
    "    # split the data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4)\n",
    "\n",
    "    # train the classifier using the training data\n",
    "    knn.fit(x_train, y_train)\n",
    "\n",
    "    # compute the prediction of the test set using the model\n",
    "    predict  = knn.predict(x_test)\n",
    "\n",
    "    accuracy[i] = accuracy_score(predict,y_test)\n",
    "\n",
    "print \"mean accuracy = %g \"%(accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Accuracy Plot')\n",
    "_ = ax1.boxplot(accuracy) \n",
    "xderiv = np.ones(accuracy[:].shape)+(np.random.rand(10,)-0.5)*0.1\n",
    "_ = plt.plot(xderiv,accuracy[:] ,'ro',alpha=0.3) # add a jigger plot of data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure if this data set is amenable to the following problem, but I thought we might try generating a figure similar to figure 5.2 for this data set.  (You would try creating this boxplot for various KNN models (with varying neighbors), support vector classifiers (SVC), and decision tree classifiers (see pg 77).  Next week, we will go into more depth regarding the SVC and decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
