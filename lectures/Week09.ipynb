{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction\n",
    "\n",
    "This lecture focuses on \"feature projection\": transforming data with many features (i.e. high-dimensional space) to a space with fewer dimensions.  In this module, we will cover:\n",
    "* Low-rank approximations (SVD) & Principal component analysis (PCA)\n",
    "* Linear Discriminant Analysis (LDA)\n",
    "* Estimating Intrinsic Dimension using nearest neighbors\n",
    "* Manifold Learning: Laplacian Eigenmaps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-rank approximations: SVD\n",
    "\n",
    "Recall from a few weeks ago (when we were studying least squares), that every data matrix, $X$ has a QR factorization, i.e., we can find matrices $Q$ and $R$ such that $A = Q\\,R$, where $Q$ is an orthogonal matrix, and $R$ is upper triangular.  If $X$ is an $m \\times n$ matrix, then \n",
    "- $Q$ is an $m \\times m$ matrix, whose columns are orthonormal to each other, i.e.:\n",
    "\n",
    "    - $q_i \\cdot q_j = 0$ if $i\\neq j$\n",
    "\n",
    "    - $q_i \\cdot q_i = 1, \\quad i=1,2,\\ldots,m$\n",
    "  \n",
    "  The matrix $Q$ has the special property tha $Q^{-1} = Q^T$.\n",
    "  \n",
    "- $R$ is an $m \\times n$ upper triangular matrix, i.e., $r_{ij} = 0$ if $i > j$.\n",
    "\n",
    "It turns out, that there is another useful factorization, called the singular value decomposition (SVD).  Specifically, every matrix $X$ also has an SVD decomposition, i.e., we can find matrices $U$,$S$ and $V$ such that $X = U\\,S\\,V^T$, where $U$ and $V$ are orthogonal matrices.  Again, if $X$ is an $m \\times n$ matrix, then\n",
    "- $U$ is an orthogonal matrix of size $m \\times m$,   \n",
    "- $S$ is a diagonal matrix with non-negative entries on the diagonal, and whose diagonal elements satisfy \n",
    "\\begin{align*}\n",
    "\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r > 0,\n",
    "\\end{align*}\n",
    "where $r = \\text{rand}(X)$ is the rank of the data matrix, i.e. the number of linearly independent rows/columns, whichever is less.\n",
    "- $V$ is an orthogonal matrix of size $n \\times n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our toy problem:  Suppose we have two packages that we wish to ship. We measure the weight of these two packages in the office. Package $A$ measures in at 2 pounds, package B measures in at $5$ pounds. At the distribution site however, the two packages are weighed together, and the joint weight is reported at 8 pounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array( [ [1,0], [0,1], [1,1]])\n",
    "print(\"our matrix X:\")\n",
    "print(X)\n",
    "y = np.array( [[2],[5],[8]])\n",
    "print(\"our target y:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[U,S,V] = np.linalg.svd(X, full_matrices = True)\n",
    "print(\"Size of U: \")\n",
    "print(U.shape)\n",
    "print(U)\n",
    "print(\"Size of S: \")\n",
    "print(S.shape)\n",
    "print(S)\n",
    "print(\"Size of V\")\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, a reduced factorization is possible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[U,S,V] = np.linalg.svd(X, full_matrices = False)\n",
    "print(\"Size of U: \")\n",
    "print(U.shape)\n",
    "print(\"Size of S: \")\n",
    "print(S.shape)\n",
    "print(\"Size of V\")\n",
    "print(V.shape)\n",
    "print(\"reconstructing X\")\n",
    "print(np.dot(U*S,V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you stares at the decomposition carefully,\n",
    "\\begin{align}\n",
    "X &= U\\,S\\,V^T \\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "u_1 | u_2 | \\cdots u_m\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 & 0 & \\cdots & &  \\\\\n",
    "0 & \\sigma_2 & 0 & & \\\\\n",
    " &  &  \\ddots & & \\\\\n",
    "& & 0 & \\sigma_n \\\\\n",
    "0 & 0 & \\cdots& 0 \\\\\n",
    "\\vdots & & & \\vdots \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_1^T\\\\\n",
    "v_2^T\\\\\n",
    "\\vdots \\\\\n",
    "v_n^T\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "you observe that the matrix can be expressed as a sum of rank-one matrices,\n",
    "\\begin{align}\n",
    "X = \\sum_{i=1}^n \\sigma_i u_i v_i^T.\n",
    "\\end{align}\n",
    "If the data matrix is of rank $r$, where $r < \\min{(m,n)}$, then $\\sigma_{r+1} = \\cdots = \\sigma_n = 0$, hence,\n",
    "\\begin{align}\n",
    "X = \\sum_{i=1}^r \\sigma_i u_i v_i^T.\n",
    "\\end{align}\n",
    "Since $\\sigma_i$ is in non-increasing, it turns out that the sum is in terms of decreasing contribution of rank-one matrices.  This leads to a notion of low-rank approximation to matrices.  The idea, is to pick a rank $k < r$ which suitably approximates the data matrix $A$.  We take the $k$ largest contributions of rank-one matrices,\n",
    "\\begin{align}\n",
    "X = \\sum_{i=1}^k \\sigma_i u_i v_i^T.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in a grayscale image and explore low-rank approximations of these images. We will use some built in images from the sci-kit family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import img_as_ubyte,img_as_float\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "img = rgb2gray(img_as_float(data.coffee()))\n",
    "\n",
    "plt.gray()\n",
    "_ = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: \n",
    "1. generate a semilog plot of the singular values as a function of index.  (see matplotlib.pyplot.semilogy)\n",
    "2. using the above information, generate various low-rank approximations to the black and white image.  Display your images using 2x2 or 3x3 grid of subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a well-known, and widely used technique for dimension reduction.  It is one of the machine learning algorithms you should know as a data scientist.  One of the first papers to introduce PCA as its known today, was published by Hotelling in 1933, http://dx.doi.org/10.1037/h0071325/.  The goal of a PCA, is to find a collection of $k$ orthogonal unit vectors, $v_1,v_2,\\ldots, v_k$ (where $k$, the intrinsic dimension, is much smaller than the ambient dimension, $n$), such that the variance of the data set, projected in the direction of $v_i$ is maximized. \n",
    "\n",
    "Lets recall some definitions we have explored previously.  We assume that our data matrix, $X$, is comprised of $m$ observations, each of $n$ features.\n",
    "\\begin{align}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x_1^T\\\\\n",
    "x_2^T \\\\\n",
    "\\vdots \\\\\n",
    "x_m^T\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "where $x_i \\in \\mathbb{R}^n$.  We can find the mean of each column, and store them in a vector $\\mu$, satisfying\n",
    "\\begin{align}\n",
    "\\mu &= \\frac{1}{m} \\sum_{i=1}^m x_i\\\\\n",
    "&= \\left(  \\frac{1}{m} \\sum_{i=1}^m x_{i1}, \\frac{1}{m} \\sum_{i=1}^m x_{i2}, \\ldots, \\frac{1}{m} \\sum_{i=1}^m x_{in}\\right)^T\n",
    "\\end{align}\n",
    "If will be convenient to refer to the data, centered about the mean as $X$, \n",
    "\\begin{align}\n",
    "X \\leftarrow X - \\mu \\otimes 1 = \n",
    "\\begin{bmatrix}\n",
    "x_1^T - \\mu^T\\\\\n",
    "x_2^T - \\mu^T\\\\\n",
    "\\vdots \\\\\n",
    "x_m^T - \\mu^T\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "The sample covariance about the mean can then be expressed:\n",
    "\\begin{align}\n",
    "S = \\frac{1}{m-1}\\sum_{i=1}^m (x_i - \\mu)(x_i - \\mu)^T = \\frac{1}{m-1}X^T X\n",
    "\\end{align}\n",
    "The size of the covariance matrix is $n\\times n$, as expected.\n",
    "\n",
    "Mathematically, we know that the projection of an arbitrary vector $x_i \\in \\mathbb{R}^n$ in the direction of $v_j$ is found using the dot product, $v_j^T x_i$.  This means, that the variance of the dataset set, projected in the direction of the first principal direction $v_1$ can be written as\n",
    "\\begin{align}\n",
    "\\frac{1}{m-1} \\sum_{i=1}^m (v_1^T x_i - v_1^T\\mu)^2 = v_1^T S\\, v_1\n",
    "\\end{align}\n",
    "We wish to maximize the above quantity, subject to $\\|v_1\\|_2 = 1$.  Using Lagrange multipliers (which you will learn in optimization theory), the solution is \n",
    "\\begin{align}\n",
    "S \\, v_1 = \\lambda_1 v_1,\n",
    "\\end{align}\n",
    "which tells us that $v_1$ must be an eigenvector of the covariance matrix $S$.  Also, since $\\|v_1\\|_2 = 1$, this allows us to conclude that the corresponding eigenvalue, $\\lambda_1$, is exactly equal to the variance of the dataset along $v_1$, i.e. $\\lambda_1 = v_1^T S\\,v_1$.  We can then continue the process, projecting the data onto $v_2$, further enforcing that $v_1$ is orthogonal to $v_2$, and then iterating through the first $k$ principal directions.  \n",
    "\n",
    "The final conclusion, is that the first $k$ principal vectors of $X$ correspond exactly to the eigenvectors of the covariance matrix, $S$, and the eigenvalues are the variance of the dataset along the corresponding eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship between SVD and PCA\n",
    "\n",
    "Recall that $X = U\\,S\\,V^T$.  So, the scaled covariance matrix can be expressed as\n",
    "\\begin{align}\n",
    "  (m-1)S = X^T X = (U\\,S\\,V^T)^T (U\\,S\\,V^T) = V(S^T S) V^T\n",
    "\\end{align}\n",
    "which means that the matrix $X^T X$ and $S^T S$ are similar, i.e., they share the same eigenvalues.  Hence, the singular values of the matrix $X$, $\\sigma_i$ are related to the eigenvalues, $\\lambda_i$, of the covariance matrix $S$ via the formula\n",
    "\\begin{align}\n",
    "\\sigma_i^2 = (m-1)\\lambda_i, \\qquad i=1,\\ldots,\\text{rank}(X)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More importantly, consider again $X = U\\,S\\,V^T$.  Since $V$ is an orthogonal matrix, we have\n",
    "\\begin{align}\n",
    "X\\,V = U\\,S\n",
    "\\end{align}\n",
    "What is the $i$th column of $X\\,V$?\n",
    "\\begin{align}\n",
    "X\\,v_i = ( (x_1-\\mu)^T v_i, \\ldots, (x_m-\\mu)^T v_i)^T,\n",
    "\\end{align}\n",
    "which is exactly the projection of the data onto the $i$th principal direction, $v_i$.  The corresponding column on the right hand side is $\\sigma_i u_i$, which is what we are trying to recover.\n",
    "\n",
    "Theoretically, one could compute the covariance matrix by forming $X^TX$, and then computing the eigen-decomposition to find the principal components.  This however turns out to be an unstable algorithm because computing $X^T\\,X$ has a lot of floating point (round-off) error.  In practice, one avoids this numerical issue by computing the SVD of the data matrix $X$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Lets use sci-kit learn to find the PCA decomposition first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# data matrix from toy package example\n",
    "X = np.array( [ [1.0,0], [0,1.0], [1,1]])\n",
    "\n",
    "pca = PCA(n_components=2) # keep both components for now\n",
    "pca.fit(X)\n",
    "print(\"singular vectors\")\n",
    "print(pca.singular_values_)  \n",
    "print(\"explained variance ratio\")\n",
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now, let's use the SVD decomposition to recover the same singular values.  We need to center the matrix about the mean.  The explained variance ratio can be computed as\n",
    "\\begin{align}\n",
    "r = \\frac{\\sigma_i^2}{\\sum \\sigma_i^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array( [ [1.0,0], [0,1.0], [1,1]])\n",
    "mu = X.mean(axis=0)\n",
    "X[:,0] = X[:,0] - mu[0]\n",
    "X[:,1] = X[:,1] - mu[1]\n",
    "\n",
    "[U,S,V] = np.linalg.svd(X, full_matrices = True)\n",
    "print(\"singular values\")\n",
    "print(S)\n",
    "print(\"explained variance ratio\")\n",
    "print(S**2/np.sum(S**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, we will now use the built-in PCA algorithm in Sci-kit learn to reduce the iris data set, and plot the various species along the largest two principal directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X[y == i, 0], X[y == i, 1], color=color, alpha=.8, lw=2,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of IRIS dataset')\n",
    "plt.xlabel(\"1st principal component\")\n",
    "_ = plt.ylabel(\"2nd principal component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "Linear discriminant analysis (LDA) is similar to PCA in that it seeks linear combination of variables to best explain the data.  However, PCA is an unsupervised technique; it does not utilize label information to construct the principle components.  If label information was available (e.g., the iris data set above), it would advantageous to utilize the label information for projection.\n",
    "\n",
    "We will tackle only the basic case where there are two label classes (e.g. cancer/healthy, fraud/legit, etc.); there are extensions to handle the multi-class variant.  The approach is to project the high-dimensional data onto a one-dimensional line in such a way that the observations in each class are as separated as possible.   This is best illustrated in an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_blobs(n_samples=20, centers = 2, random_state=2)\n",
    "colors = ['red', 'blue']\n",
    "markers = [\"o\", \"v\"]\n",
    "for color, i , mstyle in zip(colors, [0, 1], markers):\n",
    "    plt.scatter(X[y == i, 0], X[y == i, 1], color=color, alpha=.8, lw=2, marker = mstyle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "very blindly, if we project the data onto a line that is parallel to the $x$ axis, we see that the classes are not well separated.  However, if we project the data onto a line that is parallel to the $y$ axis, the classes are reasonably separated.  The goal is to fine a line so that the classes are as separated as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "for color, i , mstyle in zip(colors, [0, 1], markers):\n",
    "    plt.scatter(0.0*X[y == i, 0], X[y == i, 1], color=color, alpha=.8, lw=2, marker = mstyle)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for color, i , mstyle in zip(colors, [0, 1], markers):\n",
    "    plt.scatter(X[y == i, 0], 0.0* X[y == i, 1], color=color, alpha=.8, lw=2, marker = mstyle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to define what it means for the classes to be as separated as possible.  One natural measure of separation is to compute the difference in the mean of the projected points.  First, consider the mean of each class.  It is defined by \n",
    "\n",
    "\\begin{align}\n",
    "\\mu_i = \\frac{1}{|C_i|}\\sum_{x \\in C_i} x, \n",
    "\\end{align}\n",
    "\n",
    "where $|C_i|$ refers to the number of elements in Class $i$. \n",
    "If $v$ is the direction that we are projecting on to, the mean for each class of the *projected* data is \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mu}_i = \\frac{1}{|C_i|}\\sum_{x \\in C_i} v^T x, \n",
    "\\end{align}\n",
    "\n",
    "where we have used the hat notation to denote the projected mean.  Conveniently, we have $\\hat{\\mu} = w^T \\mu$.  Hence, the separation of the means for the two classes of projected data is:\n",
    "\n",
    "\\begin{align}\n",
    "|\\hat{\\mu}_i - \\hat{\\mu}_2| = | v^T (\\mu_1 - \\mu_2)|\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain good class separation (which will give rise to good classification performance), we want the separation of the means to be as large as possible, relative to some measure of the standard deviation for observations in each class. \n",
    "\n",
    "We will use the \"scatter\" for each class of samples,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{s}_i^2 = \\sum_{x\\in C_i} (v^T x - \\hat{\\mu}_i)^2\n",
    "\\end{align}\n",
    "\n",
    "The total within class scatter is then $\\hat{s}_1^2 + \\hat{s}_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear discriminant analysis can then be posed as an optimization problem: maximize the function $J(v)$, where\n",
    "\n",
    "\\begin{align}\n",
    "J(v) = \\frac{|\\hat{\\mu}_1 - \\hat{\\mu}_2|^2}{\\hat{s}_1^2 + \\hat{s}_2^2}\n",
    "\\end{align}\n",
    "\n",
    "With some linear algebra, the solution to the maximization problem can be written as\n",
    "\n",
    "\\begin{align}\n",
    "v = S_w^{-1} (\\mu_1 - \\mu_2)\n",
    "\\end{align}\n",
    "\n",
    "where $S_w$ is the within-class scatter matrix $S_w = S_1 + S_2$, with\n",
    "\n",
    "\\begin{align}\n",
    "S_i = \\sum_{x \\in C_i} (x - \\mu_i)(x - \\mu_i)^T\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets apply LDA to the iris example, and compare how it performs versus PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X = lda.fit(X, y).transform(X)\n",
    "\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X[y == i, 0], X[y == i, 1], color=color, alpha=.8, lw=2,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA of IRIS dataset')\n",
    "plt.xlabel(\"1st LDA component\")\n",
    "_ = plt.ylabel(\"2nd LDA component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wraps up a quick survey of linear dimension-reduction methods. \n",
    "- PCA: most commonly used; does not take classes into account\n",
    "- LDA: also popular, takes classes into account.\n",
    "\n",
    "Other approaches to linear dimension reduction include:\n",
    "- non-negative matrix factorization (NMF).  If your data matrix is all positive, and one desires non-negative features, it is desirable to have a method for reducing dimension that is guaranteed to produce non-negative features.  idea: constrained optimization. \n",
    "- factor analysis: idea is to define latent variables (or common factors) and assume that the data features can be represented as a linear combination of these latent variables.  \n",
    "- random projections: sometimes, the data set is so large (or expensive to access), that using the SVD or PCA is intractable.  Utilizing tools from randomized numerical linear algebra techniques (rNLA), one can use random projections to obtain a low-rank approximation to a data matrix  or it's inverse.   \n",
    "\n",
    "Before venturing into the nonlinear dimension reduction realm (a.k.a. manifold learning), we first discuss how to estimate the intrinsic dimension of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Intrinsic Dimension\n",
    "\n",
    "Vocabulary: we often refer to the \"ambient\" dimension, as the number of features/dimension of our data.  When people refer to big data, they often refer to a large number of samples and/or a large number of features (i.e. large ambient dimension).\n",
    "\n",
    "As we have seen, data can sometimes be well approximated in lower dimension (see SVD discussion above).  Intrinsic dimension is defined as the smallest number of dimensions (or variables) needed to account for the observed properties of the data.\n",
    "\n",
    "Modern approaches for estimating the intrinsic dimension is categorized as being *local* or *global*.\n",
    "- local approaches estimate the dimensionality using information obtained from neighborhoods of the observations\n",
    "- global approaches utilize the global properties of the data (e.g. the amount of variance explained by PCA).\n",
    "\n",
    "We will explore one local estimator: the nearest neighbor approach.  Other local approaches include: computing the correlation dimension, computing the maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Approach\n",
    "\n",
    "The following discussion is loosely based on Pettis et. al (1979) http://dx.doi.org/10.1109/TPAMI.1979.4766873.  Let $r_{k,i}$ be the distance between sample $i$ and it's $k$th nearest neighbor.  We then define the average $k$th nearest neighbor distance as \n",
    "\n",
    "\\begin{align}\n",
    "\\bar{r} = \\frac{1}{m}\\sum_{i = 1}^m r_{k,i}\n",
    "\\end{align}\n",
    "\n",
    "Using probability arguments, Petis et. al (1979) show that the expected value of the average distance, $E(\\bar{r})$ satisfies\n",
    "\n",
    "\\begin{align}\n",
    "  E(\\bar{r}) = \\frac{1}{G_{k,d}} k^{1/d} C_m,\n",
    "\\end{align}\n",
    "\n",
    "where $d$ is the intrinsic dimension we seek, $C_m$ is some constant that does not depend on $d$, and $G_{k,d}$ is a complicated looking term involving falling factorials,\n",
    "\n",
    "\\begin{align}\n",
    "  G_{k,d} = \\frac{k^{1/d} \\Gamma(k)}{\\Gamma(1 + k/d)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some algebra and using properties of logs, one can arrive at the equation\n",
    "\\begin{align}\n",
    "\\log{(G_{k,d})} + \\log{E(\\bar{r})} = \\frac{1}{d}\\log k + \\log{C_m},\n",
    "\\end{align}\n",
    "\n",
    "which can be solved using an iterative formula proposed by Pettis et. al.  At each step of the iteration, one solves a regression problem, fitting the above equation to data collected for various $k$-nearest neighbor measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning - brief survey\n",
    "\n",
    "Often, data is not presented well by linear combinations of the features; visually, a hyperplane does not describe the data well. The alternative is non-linear dimension reduction (NLDR); visually, this tries to find a (non-linear) manifold that approximates the data well.\n",
    "\n",
    "Sci-kit learn has various built-in approaches for manifold learning, http://scikit-learn.org/stable/modules/manifold.html, along with a more thorough description of each method. \n",
    "We give a brief summary of some of them, as well as a few others which are not listed. \n",
    "\n",
    "- Multi-dimensional Scaling (MDS) seeks to identify hidden structure in the data by finding configuration of data points in a low-dimensional space such that the Euclidean distance between points in the projected space represents, to some degree, the proximity between points in the high-dimensional space.  \n",
    "\n",
    "- Isomap (Isometric mapping methods), is an extension of MDS; whereas MDS uses pairwise Euclidean distance to create the low-dimensional embedding, Isomap measures the geodesic distance induced by the graph of the neighborhood.\n",
    "\n",
    "- Locally Linear Embedding (LLE), for each data sample, one finds the nearest neighbors and uses eigenvector-based optimization to express the point as a linear combination of its neighbors.  (i.e. we generate a piecewise linear manifold)\n",
    "\n",
    "- Laplacian Eigenmaps (Belkin and Niyogi, NIPS 2001). We will explore this approach in this module, as it provides a theoretical framework for understanding LLE.  As part of the project, will be reading this seminal paper by Belkin and Niyogi (2003) http://web.cse.ohio-state.edu/~belkin.8/papers/LEM_NC_03.pdf.\n",
    "\n",
    "- Hessian eigenmaps (Donoho and Grimes, 2003); this approach attempts to minimize the local curvature of the embedding manifold.\n",
    "\n",
    "- Spectral Embedding - this projects the data onto the eigenvectors of the graph Laplacian. Sci-kit actually implements the Laplacian Eigenmaps when the spectral embedding function is called.\n",
    "\n",
    "- t-distributed Stochastic Neighbor Embedding (t-SNE). This method models each high-dimensional data sample as a two- or three-dimensional object in such a way that similar objects are modeled by nearby low-dimensional points, and dissimilar objects are modeled by distant low-dimensional points, with high probability. \n",
    "\n",
    "- charting (Brand, NIPS 2002): uses density estimation to learn the manifold.  This approach is more robust to noisy or sparsely sampled data.\n",
    "\n",
    "### Comparisons\n",
    "\n",
    "There are a few examples on sci-kit learn that illustrate how various NLDR techniques behave.\n",
    "- [reducing the S-curve](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py)\n",
    "- [the severed sphere](http://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py)\n",
    "- [hand written digits](http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian Eigenmaps\n",
    "\n",
    "The general idea is as follows.  We let $x_1,\\ldots,x_m$ denote our $m$ observations in $\\mathbb{R}^n$.  The algorithm seeks to generate a weighted graph with these $m$ nodes, one for each sample, and a set of edges that connect to other samples.  The low-dimensional manifold (embedding map) is then found by computing the eigenvalues of the graph Laplacian, and retrieving the surface constructed using the smallest eigenvalues.  The theoretical justification is beyond the scope of this course, but the interested reader can look at Belkin and Niyogi (2003).\n",
    "\n",
    "The algorithm has 3 main steps:\n",
    "\n",
    "1. Construct the graph\n",
    "2. Compute the graph Laplacian\n",
    "3. Embed data using the graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: construct the similarity graph.\n",
    "\n",
    "The similarity graphs are meant to model the relationship between data samples in a *local* neighborhood.  In general, a graph is fully described by vertices, edges and weights.  \n",
    "- the vertices here are obvious: all the data points\n",
    "- there are several approaches on how to construct the edges \n",
    "    1. put an edge between 2 data samples if they are within $\\epsilon$ distance of each other\n",
    "    2. put an edge between 2 data points if they are k-nearest-neighbors of each other. Note, this definition can be generalized to cases where $x_i$ can be a neighbor of $x_j$ but not vice versa.  This can lead to three types of graphs:\n",
    "        - directed graph: add an edge, $x_i \\rightarrow x_j$ if $x_i$ is a kNN of $x_j$.\n",
    "        - symmetric graph: add edge, $x_i - x_j$ if ($x_i$ is a kNN of $x_j$) *OR* ($x_j$ is a kNN of $x_i$).\n",
    "        - mutual graphL add edge, $x_i - x_j$ if ($x_i$ is a kNN of $x_j$) *AND* ($x_j$ is a kNN of $x_i$). \n",
    "- weights.  Many choices here.  Most often:\n",
    "    1. equal weighting.  Set $W_{ij} = 1$ if an edge exists between $x_i$ and $x_j$\n",
    "    2. weight based on distance, e.g., \n",
    "    \\begin{align}\n",
    "    W_{ij} = \\exp{\\left(\\frac{\\|x_i - x_j\\|_2^2}{2 t^2}\\right)}\n",
    "    \\end{align}\n",
    "    where $t$ is a tuning parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some implementation choices that are important when generating the similarity graph.  The first obvious choice is on how to construct the edge.  $\\epsilon$-neighborhood, or kNN.  Once you've picked one, you then have to decide either how to choose $\\epsilon$, or how to choose $k$.  This is an art.  The goal is to preserve local information, so you don't want to choose neighborhood sizes that are too large; the flip side, is you want to choose $\\epsilon$ and $k$ large enough so that you capture dependency of data with each other.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute the graph Laplacian\n",
    "\n",
    "The graph Laplacian, $L$, of the constructed graph is defined as $L = D - W$, where\n",
    "- $W$ is the weight matrix, size $m\\times m$, defined above.\n",
    "- $D$ is the degree matrix.  The degree matrix is a diagonal matrix of size $m\\times m$, and whose entries $d_{ii}$ is the degree of vertex $i$, i.e., the sum of all weights that are connected to vertex $i$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Embed data using graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the eigenvalues and eigenvectors of the graph Laplacian\n",
    "- if a $d$ dimensional projection is desired, project data onto eigenvectors associated with $d$ smallest eigenvalues.  Intuition question: why $d$ smallest eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of your next project will be to apply Laplacian Eigenmaps to the swiss-roll data set.  The goal is to recover the Laplacian eigenmaps associated with the swiss roll:\n",
    "[Laplacian Eigenmap for Swiss Roll](figures/EigenMap-SwissRoll.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
