{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "Recommender systems are widely used in online commerce; the system processes information about the user, their networks, and the products/services, and subtly suggests personalized products or services that the user might purchase or click on.  For example, my Amazon account is always suggesting items that I might be interested in, Netflix has a list of shows that I might want to watch, Facebook and Google Ads customize the ads seen based on the user.  Under the hood is a recommendation system that is analyzing users' behaviors and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe there is an even bigger role for recommender systems to play in the coming years due to the vast volume of data being collected.  In Dec 2014, Youtube claimed 300 hours of content was uploaded every minute.  In July 2015, over 400 hours of content was uploaded every minute.  Many people now have access to multiple cameras capable of recording images or streaming videos (just think of it: every smart phone/tablet has multiple cameras -- there might even be more cameras than there are people on earth!) Question: how do we gain insight from this swath of data?  Ideally, our supervised learning and un-supervised learning algorithms will be able to help us classify objects, actions, moods, but this is a long way off!  Perhaps, a better target might be a hybrid recommender -- supervised learning approach, where a human starts labelling, a recommender starts learning and aids the labelling, and eventually transitions to an un-supervised learning solution ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "\n",
    "There are four approaches to designing recommender systems\n",
    "- Simple Recommender\n",
    "- Content-based filtering\n",
    "- Collaborative filtering\n",
    "- Hybrid filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Recommenders\n",
    "\n",
    "Simple recommenders, as the name suggests, offer generalized recommendations to every user based on some global ranking system.  For example, if one could parse the ranking of items on amazon.com and make recommendations to the shopper based on the ranking of the particular class of items.  There are some subtleties in making a recommendation however: are you more likely to go with an item that has an average item rating of 4.5 out of five stars, with 100,000 reviews, or are you more likely to go with an item that has an average rating of 4.8 based on six reviews?  Intuitively, as the number of voters increase, the rating of an item approaches a value that is reflective of the item's quality. It is more difficult to discern the quality of an item with only a few voters.\n",
    "\n",
    "One solution is to use a weighted rating, i.e., weight the rating based on how many reviews were given.\n",
    "\\begin{align}\n",
    "\\hat{r} = \\frac{n\\,r}{n+m} + \\frac{m\\,\\bar{r}}{n+m}\n",
    "\\end{align}\n",
    "where\n",
    "- $n$ is the number of votes for the item\n",
    "- $r$ is the average rating for the item\n",
    "- $m$ is the minimum number of votes required for a recommendation\n",
    "- $\\bar{r}$ is the mean rating over all items being considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Lets look at an example: the MovieLens dataset, http://dx.doi.org/10.1145/2827872, which was part of the NetFlix prize in 2009.  This data was collected by the GroupLens Research Project at the University of Minnesota.\n",
    "The data consists of:\n",
    "- 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "- Each user has rated at least 20 movies. \n",
    "- Simple demographic info for the users (age, gender, occupation, zip)\n",
    "\n",
    "A larger data set (45,000 movies, 26 million ratings, 270,000 users) is available here:\n",
    "https://www.kaggle.com/rounakbanik/the-movies-dataset/data#movies_metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('data/ml-100k/ratings.dat', sep='\\t', names=r_cols)\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets drop the timestamp, since we are not going to make use of it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.drop(columns=['unix_timestamp'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now, group by movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = ratings.groupby('movie_id')['rating'].agg(['sum','count','mean'])\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also decide $m$, the minimum number of ratings needed for a recommendation.  The 6 votes is the 25 percentil.  Lets drop all movies with less than 6 votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 6\n",
    "grouped.drop(grouped[grouped['count']<m].index,inplace=True)\n",
    "grouped.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, lets find the mean rating of this group of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbar = float(grouped['sum'].sum())/grouped['count'].sum()\n",
    "print \"rbar = \", rbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets add a new column that computes the weighted mean for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped['weighted mean'] = grouped['count']*grouped['mean']/(grouped['count'] + m) \\\n",
    "+ m*rbar / (grouped['count'] + m) \n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which are the most highly rated movies that we can recommend.  First, let's load the movie information into another dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cols = ['movie_id', 'title', 'release_date']\n",
    "movie_info = pd.read_csv('data/ml-100k/movie-info.dat', sep='|', names=m_cols, usecols=range(3))\n",
    "movie_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now merge this dataframe with the grouped data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(grouped,movie_info,on=\"movie_id\",how=\"inner\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and sort by weighted mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.sort_values(by=['weighted mean'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-based filtering\n",
    "\n",
    "Content-based filtering methods are based on a profile of a user, e.g. a user rates an object or clicks on a link.  Based on that data, the user profile is generated, which is then used to make suggestions to the user.  As the user provides more inputs or takes actions on the recommendations, the profile (and consequently the recommender system) becomes more accurate.  Content-based filtering assumes that information about the content is available and can be easily retrieved.  This is an increasingly difficult problem as the amount of content (e.g. number of items that is available for purchase on Amazon) is increasing.  There are entire classes based on information retrieval. Here are some pros and cons of content-based filtering.\n",
    "\n",
    "Pros:\n",
    "- ability to make recommendations o users with unique profiles\n",
    "- ability to recommend new and unpopular items\n",
    "- independent of other users\n",
    "- can specify which content-features caused an item to be recommended\n",
    "\n",
    "Cons:\n",
    "- requires building a user profile.  What do we do for new users?\n",
    "- \"over-fitting\": never recommends items outside of user's profile\n",
    "- unable to exploit judgements of other users\n",
    "- people might have multiple interests\n",
    "- content filters need be available and accurate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "This section is based on the [data camp tutorial](https://www.datacamp.com/community/tutorials/recommender-systems-python) on content-based filtering.  In this example, we wish to make a movie recommendation based on a plot description.  We will switch to the kaggle data set which has a column for the plot description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('data/movies_metadata.csv', low_memory=False)\n",
    "metadata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, lets drop all movies with less than 6 votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 6\n",
    "metadata.drop(metadata[metadata['vote_count']<m].index,inplace=True)\n",
    "metadata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print plot overviews of the first 5 movies.\n",
    "metadata['overview'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to generate a document-term matrix, that is, a matrix that describes the frequency at which terms appears in a document, or in this case, the plot overview.  In a document-term matrix, each row corresponds to a document (in this case, plot overview of a movie), each column corresponds to a term.  There is a built-in function in Scikit-Learn to generate this document-term matrix.  In the machine learning realm, the document-term matrix is referred to as Term Frequency-Inverse Document Frequency (TF-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import TfIdfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Replace NaN with an empty string\n",
    "metadata['overview'] = metadata['overview'].fillna('')\n",
    "\n",
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(metadata['overview'])\n",
    "\n",
    "#Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, 57k words are used to describe the 28k movies.  With this document-term matrix, we can now compute a similarity score.  We have seen similarity measures when we were exploring clustering.  Some common ones are:\n",
    "- the Euclidean distance, $\\|\\vec{x}-\\vec{y}\\|_2$\n",
    "- the cosine similarity (the cosine of an angle between two vectors)\n",
    "\\begin{align}\n",
    "\\cos{\\theta} = \\frac{\\vec{x}\\cdot\\vec{y}}{\\|x\\|_2 \\|y\\|_2}\n",
    "\\end{align}\n",
    "- Pearson correlation coefficient\n",
    "\\begin{align}\n",
    "\\rho = \\frac{(\\vec{x}-\\vec{\\bar{x}})\\cdot(\\vec{y}-\\vec{\\bar{y}})}{\\|\\vec{x}-\\vec{\\bar{x}}\\|_2\\|\\vec{y}-\\vec{\\bar{y}}\\|_2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can create our recommender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a reverse map of indices and movie titles\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('Toy Story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('Kung Fu Panda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, not that great.  I guess the plots are somewhat the same, but ...  One can create more sophisticated content-based recommendation systems by invoking genres, keywords, actors and actresses, directors, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering\n",
    "\n",
    "The third type of recommender system, is collaborative based filtering.  Instead of relying on a user profile and content of items to be recommended, a user's network is utilized to identify users that have similar profiles, and make recommendations based on those user's profile.  \n",
    "\n",
    "Pros:\n",
    "- don't need to identify features of the items\n",
    "\n",
    "Cons:\n",
    "- tends to recommend popular items, making it hard to recommend items to someone with unique tastes (popularity bias)\n",
    "- “the cold start problem”, system is not able to give recommendations for users who have no (or very little) usage activity, aka new user problem, or recommend new items for which there is no (or very little) usage activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recommender system is covered in your textbook, chapter 9.  Here are the key steps to create a collaborative recommender system.  We have to create: \n",
    "- a predictor function, \n",
    "- a user-similarity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function:\n",
    "\n",
    "The prediction function behind the collaborative filtering is based on the movie ratings from similar users.  In order to recommend a movie, $p$, from a set of movies, $P$, to a given user, $a$, we first need to see the set of users, $B$, who have already seen $p$. Then, we need to see the taste similarity between these users in $B$ and user $a$. The most simple prediction function for a user $a$ and movie $p$ can be defined as follows:\n",
    "\n",
    "$$pred(a,p) = \\frac{\\sum_{b \\in B}{sim(a,b)*(r_{b,p})}}{\\sum_{b \\in B}{sim(a,b)}}$$\n",
    "\n",
    "where $sim(a,b)$ is the similarity between user $a$ and user $b$,  $B$ is the set of users in the dataset that have already seen $p$ and $r_{b,p}$ is the rating of $p$ by $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity function\n",
    "\n",
    "\n",
    "We need to first identify the set of ratings for all movies common to two users before we can compute the user similarity. We have already mentioned three popular similarity functions: Euclidean Distance, Pearson Correlation Distance, Cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here is a simple example, taken from https://github.com/mukesh-srivastav/MovieRecommenderSystem, that illustrates how we can find similar users and then give a simple recommendation.   Your text has a more complicated example using the movielens database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Dictionary of movie critics and their ratings of a small set of movies\n",
    "critics={'Lisa Rose': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n",
    " 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,\n",
    " 'The Night Listener': 3.0},\n",
    "'Gene Seymour': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,\n",
    " 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,\n",
    " 'You, Me and Dupree': 3.5},\n",
    "'Michael Phillips': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n",
    " 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n",
    "'Claudia Puig': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n",
    " 'The Night Listener': 4.5, 'Superman Returns': 4.0,\n",
    " 'You, Me and Dupree': 2.5},\n",
    "'Mick LaSalle': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n",
    " 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n",
    " 'You, Me and Dupree': 2.0},\n",
    "'Jack Matthews': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n",
    " 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n",
    "'Toby': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Lisa Rose'' rating of \"Lady i the Water: \"', critics['Lisa Rose']['Lady in the Water']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build custom function to compute Pearson correlation coefficient for this simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the Pearson correlation coefficient for p1 and p2\n",
    "def sim_pearson(prefs,p1,p2):\n",
    "    \n",
    "    # Get the list of mutually rated items\n",
    "    mutual={}\n",
    "    for item in prefs[p1]:\n",
    "        if item in prefs[p2]: \n",
    "            mutual[item]=1\n",
    " \n",
    "    # Find the number of elements\n",
    "    n=len(mutual)\n",
    "     \n",
    "    # if they are no ratings in common, return 0\n",
    "    if n==0: return 0\n",
    " \n",
    "    # Add up all the preferences\n",
    "    sum1=sum([prefs[p1][movie] for movie in mutual])\n",
    "    sum2=sum([prefs[p2][movie] for movie in mutual])\n",
    " \n",
    "    # Sum up the squares\n",
    "    sum1Sq=sum([pow(prefs[p1][movie],2) for movie in mutual])\n",
    "    sum2Sq=sum([pow(prefs[p2][movie],2) for movie in mutual])\n",
    " \n",
    "    # Sum up the products\n",
    "    pSum=sum([prefs[p1][movie]*prefs[p2][movie] for movie in mutual])\n",
    "\n",
    "    # Calculate Pearson score\n",
    "    num=pSum-(sum1*sum2/n)\n",
    "    den=sqrt((sum1Sq-pow(sum1,2)/n)*(sum2Sq-pow(sum2,2)/n))\n",
    " \n",
    "    if den==0: return 0\n",
    "\n",
    "    r=num/den\n",
    " \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sim_pearson(critics,'Lisa Rose','Gene Seymour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the best matches for person from the prefs dictionary.\n",
    "# Number of results and similarity function are optional params.\n",
    "def topMatches(prefs,person,n=5,similarity=sim_pearson):\n",
    "    scores=[(similarity(prefs,person,other),other)\n",
    "    for other in prefs if other!=person]\n",
    "\n",
    "    # Sort the list so the highest scores appear at the top\n",
    "    scores.sort( )\n",
    "    scores.reverse( )\n",
    "    return scores[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topMatches(critics,'Toby',n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create recommendation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets recommendations for a person by using a weighted average\n",
    "# of every other user's rankings\n",
    "def getRecommendations(prefs,person,similarity=sim_pearson):\n",
    "    totals={}\n",
    "    simSums={}\n",
    "    for other in prefs:\n",
    "        # don't compare me to myself\n",
    "        if other==person: continue\n",
    "        \n",
    "        sim=similarity(prefs,person,other)\n",
    "     \n",
    "        # ignore scores of zero or lower\n",
    "        if sim<=0: continue\n",
    "        for item in prefs[other]:\n",
    "            # only score movies I haven't seen yet\n",
    "            if item not in prefs[person] or prefs[person][item]==0:\n",
    "            \n",
    "                # Similarity * Score\n",
    "                totals.setdefault(item,0)\n",
    "                totals[item]+=prefs[other][item]*sim\n",
    "                \n",
    "                # Sum of similarities\n",
    "                simSums.setdefault(item,0)\n",
    "                simSums[item]+=sim\n",
    "        \n",
    "    # Create the normalized list\n",
    "    rankings=[(total/simSums[item],item) for item,total in totals.items( )]\n",
    " \n",
    "    # Return the sorted list\n",
    "    rankings.sort( )\n",
    "    rankings.reverse( )\n",
    "\n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getRecommendations(critics,'Toby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get a recommendation based on movie,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformPrefs(prefs):\n",
    "    result={}\n",
    "    for person in prefs:\n",
    "        for item in prefs[person]:\n",
    "            result.setdefault(item,{})\n",
    "            # Flip item and person\n",
    "            result[item][person]=prefs[person][item]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=transformPrefs(critics)\n",
    "topMatches(movies,'Superman Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
