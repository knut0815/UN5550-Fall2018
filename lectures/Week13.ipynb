{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "When people refer to deep learning, they are often referring to a class of machine learning algorithms known as neural networks, which was inspired by the structure and function of the brain.  Whereas the performance of older machine learning algorithms saturates as more data is available, deep learning algorithms have the capability to sustain performance improvement.  (see slide 30 of Andrew Ng's 2015 ExtractConf talk on \"What data scientists should know about deep learning\", https://www.slideshare.net/ExtractConf)\n",
    "\n",
    "Material for this lecture was taken partially from:\n",
    "- https://www.deeplearningbook.org/\n",
    "- https://web.stanford.edu/class/cs20si/2017\n",
    "- https://github.com/chiphuyen/stanford-tensorflow-tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "TensorFlow was originally created by researchers from the Google Brain team for machine learning, but was later open sourced under the Apache 2.0 License in 2015.  It is intended to be a software library for \"high performance numerical computations\" (https://www.tensorflow.org).  Because the flexible architecture allows it to be deployed across many different platforms (gpus, cpus, rasberry Pi, ...) and the growing community utilizing and contributing to tensorflow (Google, NVIDIA, Intel, eBay, Uber, Dropbox ...), it is the defacto production level machine learning library.\n",
    "\n",
    "## Keras?\n",
    "\n",
    "How does Keras fit into the picture?  Well, there are many high level APIs built on top of TensorFlow, including Keras, TFLearn and Sonner.  These are meant for rapid prototyping.  If you need flexibility to define something not supported in an API, e.g., hybrid auto-encoders, then one returns to the low-level libraries provided by Tensorflow.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow: What's in a name?\n",
    "\n",
    "Tensor: a generalization of vectors and matrices.  Tensors are n-dimensional arrays of some data-type.  The main benefit behind adopting tensors are that most tensor operations are pleasantly parallel, and are thus scalable to large problems.  \n",
    "\n",
    "Flow: in TensorFlow, a computation is described using a Data Flow graph, where each node in the graph represents the instance of a mathematical operation, and each edge is a tensor on which the operation is performed.\n",
    "\n",
    "Hence, since TensorFlow adopts a computation graph approach, any TensorFlow program consists of two stages:\n",
    "- Stage 1: assembly of a graph\n",
    "- Stage 2: executing operations in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding two rank-0 tensors\n",
    "\n",
    "# building a dataflow graph\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "x = tf.add(a,b)\n",
    "\n",
    "# constructing a session to execute the graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# execute graph and store value in result\n",
    "result = sess.run(x)\n",
    "\n",
    "print \"sum of the rank-0 tensors is \", result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtracting two rank-2 tensors\n",
    "\n",
    "# building a dataflow graph\n",
    "a = tf.constant([[1,2],[3,4]])\n",
    "b = tf.constant([[3,4],[5,6]])\n",
    "x = tf.subtract(a,b)\n",
    "\n",
    "# constructing a session to execute the graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# execute graph and store value in result\n",
    "result = sess.run(x)\n",
    "\n",
    "print \"difference of the rank-2 tensors is \", result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the absolute value of a rank-3 tensor \n",
    "\n",
    "# building a dataflow graph\n",
    "a = tf.constant(np.arange(-6, 6, dtype=np.int32),\n",
    "                shape=[3, 2, 2])\n",
    "x = tf.abs(a)\n",
    "\n",
    "# constructing a session to execute the graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# execute graph and store value in result\n",
    "result = sess.run(x)\n",
    "\n",
    "print \"absolute value of this rank-3 tensor is \", result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full list of math operations in tensor flow is here:\n",
    "https://github.com/tensorflow/docs/blob/master/site/en/api_guides/python/math_ops.md\n",
    "\n",
    "Some things to be aware of:\n",
    "- unlike NumPy or Python sequences, TensorFlow sequences are not iterable\n",
    "i.e.\n",
    "```python\n",
    "for _ in tf.range(4): # TypeError: 'Tensor' object is not iterable.    \n",
    "```\n",
    "- There are many different operations for division.  (div, divide, truediv, floordiv, realdiv ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like NumPy, TensorFlow has its own native data types.  A full list is here: https://www.tensorflow.org/api_docs/python/tf/dtypes/DType\n",
    "- tf.float16: 16-bit half-precision floating-point.\n",
    "- tf.float32: 32-bit single-precision floating-point.\n",
    "- tf.float64: 64-bit double-precision floating-point.\n",
    "- tf.bfloat16: 16-bit truncated floating-point.\n",
    "- tf.complex64: 64-bit single-precision complex.\n",
    "- tf.complex128: 128-bit double-precision complex.\n",
    "- tf.int8: 8-bit signed integer.\n",
    "- tf.uint8: 8-bit unsigned integer.\n",
    "- tf.uint16: 16-bit unsigned integer.\n",
    "- tf.uint32: 32-bit unsigned integer.\n",
    "- tf.uint64: 64-bit unsigned integer.\n",
    "- tf.int16: 16-bit signed integer.\n",
    "- tf.int32: 32-bit signed integer.\n",
    "- tf.int64: 64-bit signed integer.\n",
    "- tf.bool: Boolean.\n",
    "- tf.string: String."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "So far, you will notice that I have only applied TensorFlow operations to \"constants\", i.e., I have allocated some memory space for a tensor, and assigned values to the memory space.  These memory spaces stay \"constant\" throughout a TensorFlow session.  Often however, we will want to update values (e.g. weights) inside of a TensorFlow computation.  Whereas a constant's value is stored in a graph and replicated whenever a graph is loaded, a variable is stored separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a variable as 2, overwriting it with it's value * 2\n",
    "\n",
    "# building a dataflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# create a variable whose original value is 2\n",
    "a = tf.get_variable('scalar', initializer=tf.constant(2)) \n",
    "a_times_two = a.assign(a * 2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    o1 = sess.run(a_times_two) # >> 4\n",
    "    o2 = sess.run(a_times_two) # >> 8\n",
    "    o3 = sess.run(a_times_two) # >> 16\n",
    "    \n",
    "print \"o1 = \", o1\n",
    "print \"o2 = \", o2\n",
    "print \"o3 = \", o3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Consider the following data set, that gives the birth rate and life expectency for various countries, data/birth_life_2010.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_birth_life_data(filename):\n",
    "    \"\"\"\n",
    "    Read in birth_life_2010.txt and return:\n",
    "    data in the form of NumPy array\n",
    "    n_samples: number of samples\n",
    "    \"\"\"\n",
    "    text = open(filename, 'r').readlines()[1:]\n",
    "    data = [line[:-1].split('\\t') for line in text]\n",
    "    births = [float(line[1]) for line in data]\n",
    "    lifes = [float(line[2]) for line in data]\n",
    "    data = list(zip(births, lifes))\n",
    "    n_samples = len(data)\n",
    "    data = np.asarray(data, dtype=np.float32)\n",
    "    return data, n_samples\n",
    "\n",
    "\n",
    "# read in data\n",
    "data, n_samples = read_birth_life_data(\"data/birth_life_2010.txt\")\n",
    "\n",
    "print \"number of samples:  \", n_samples\n",
    "print \"size of data: \", data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1],marker='.')\n",
    "plt.xlabel('birth rate')\n",
    "_ = plt.ylabel('life expectency (years)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the TensorFlow regression code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data flow graph.\n",
    "\n",
    "# create placeholders for X (birth rate) and Y (life expectancy)\n",
    "# a.k.a. define variable without initializing them.\n",
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')\n",
    "\n",
    "\n",
    "# create slope and intercep, initialized to 0\n",
    "m = tf.get_variable('slope', initializer=tf.constant(0.0))\n",
    "b = tf.get_variable('intercept', initializer=tf.constant(0.0))\n",
    "\n",
    "# construct model to predict Y (life expectancy from birth rate)\n",
    "Y_predicted = m * X + b \n",
    "\n",
    "# use squared error as the function to minimize\n",
    "loss = tf.square(Y - Y_predicted)\n",
    "\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "# in neural networks: learning rate controls how much we are adjusting weight of network\n",
    "# with respect to gradient loss.  (a.k.a. maximum step size)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    " \n",
    "with tf.Session() as sess:\n",
    "    # initialize the necessary variables, in this case, m and b\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    \n",
    "    # train the model\n",
    "    for i in range(100): # run 100 epochs\n",
    "        for x, y in data:\n",
    "            # Session runs train_op to minimize loss\n",
    "            sess.run(optimizer, feed_dict={X: x, Y:y}) \n",
    "\n",
    "    # output the values of m and b\n",
    "    m_out, b_out = sess.run([m, b]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(1,8)\n",
    "Y_pred = m_out*X_test + b_out\n",
    "plt.scatter(data[:,0], data[:,1],marker='.')\n",
    "plt.plot(X_test,Y_pred,color='blue',linewidth=3)\n",
    "plt.xlabel('birth rate')\n",
    "_ = plt.ylabel('life expectency (years)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one could fit different models quite easily, for example, a quadratic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data flow graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# create placeholders for X (birth rate) and Y (life expectancy)\n",
    "# a.k.a. define variable without initializing them.\n",
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')\n",
    "\n",
    "\n",
    "# create slope and intercep, initialized to 0\n",
    "a = tf.get_variable('quadratic_coeff', initializer=tf.constant(0.0))\n",
    "b = tf.get_variable('linear_coeff', initializer=tf.constant(0.0))\n",
    "c = tf.get_variable('constant_coeff', initializer=tf.constant(0.0))\n",
    "\n",
    "# construct model to predict Y (life expectancy from birth rate)\n",
    "Y_predicted = a * X * X + b * X + c \n",
    "\n",
    "# use squared error as the function to minimize\n",
    "loss = tf.square(Y - Y_predicted)\n",
    "\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "# in neural networks: learning rate controls how much we are adjusting weight of network\n",
    "# with respect to gradient loss.  (a.k.a. maximum step size)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    " \n",
    "with tf.Session() as sess:\n",
    "    # initialize the necessary variables, in this case, m and b\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    \n",
    "    # train the model\n",
    "    for i in range(100): # run 100 epochs\n",
    "        for x, y in data:\n",
    "            # Session runs train_op to minimize loss\n",
    "            sess.run(optimizer, feed_dict={X: x, Y:y}) \n",
    "\n",
    "    # output the values of m and b\n",
    "    a_out, b_out, c_out = sess.run([a, b , c]) \n",
    "    \n",
    "X_test = np.linspace(1,7,100)\n",
    "Y_pred = a_out*X_test**2 + b_out * X_test + c_out\n",
    "plt.scatter(data[:,0], data[:,1],marker='.')\n",
    "plt.plot(X_test,Y_pred,color='blue',linewidth=3)\n",
    "plt.xlabel('birth rate')\n",
    "_ = plt.ylabel('life expectency (years)')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clearly, not a great fit, but it was easy to specify a model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've used TensorFlow before, you're probably rolling your eyes because this simplistic example stores data in a non-TensorFlow object, namely, a NumPy array, and then feed it into TensorFlow using feed_dict.  One should instead use tf.data, where specifically, we store it in a tf.data.Dataset object.  Lets do some benchmarks. Back to linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data flow graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# create placeholders for X (birth rate) and Y (life expectancy)\n",
    "# a.k.a. define variable without initializing them.\n",
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')\n",
    "\n",
    "\n",
    "# create slope and intercep, initialized to 0\n",
    "m = tf.get_variable('slope', initializer=tf.constant(0.0))\n",
    "b = tf.get_variable('intercept', initializer=tf.constant(0.0))\n",
    "\n",
    "# construct model to predict Y (life expectancy from birth rate)\n",
    "Y_predicted = m * X + b \n",
    "\n",
    "# use squared error as the function to minimize\n",
    "loss = tf.square(Y - Y_predicted)\n",
    "\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "# in neural networks: learning rate controls how much we are adjusting weight of network\n",
    "# with respect to gradient loss.  (a.k.a. maximum step size)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    " \n",
    "start = time.time()    \n",
    "with tf.Session() as sess:\n",
    "    # initialize the necessary variables, in this case, m and b\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    \n",
    "    # train the model\n",
    "    for i in range(100): # run 100 epochs\n",
    "        for x, y in data:\n",
    "            # Session runs train_op to minimize loss\n",
    "            sess.run(optimizer, feed_dict={X: x, Y:y}) \n",
    "\n",
    "    # output the values of m and b\n",
    "    m_out, b_out = sess.run([m, b]) \n",
    "    \n",
    "print('Computation took: %f seconds' %(time.time() - start))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets now export the data to the tf.data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data flow graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "X, Y = iterator.get_next()\n",
    "\n",
    "# create slope and intercep, initialized to 0\n",
    "m = tf.get_variable('slope', initializer=tf.constant(0.0))\n",
    "b = tf.get_variable('intercept', initializer=tf.constant(0.0))\n",
    "\n",
    "# construct model to predict Y (life expectancy from birth rate)\n",
    "Y_predicted = m * X + b \n",
    "\n",
    "# use squared error as the function to minimize\n",
    "loss = tf.square(Y - Y_predicted)\n",
    "\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "# in neural networks: learning rate controls how much we are adjusting weight of network\n",
    "# with respect to gradient loss.  (a.k.a. maximum step size)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    " \n",
    "start = time.time()    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # initialize the necessary variables, in this case, m and b\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    \n",
    "    # train the model\n",
    "    for i in range(100): # run 100 epochs\n",
    "        sess.run(iterator.initializer) # initialize the iterator\n",
    "   \n",
    "    # output the values of m and b\n",
    "    m_out, b_out = sess.run([m, b])\n",
    "    \n",
    "print('Computation took: %f seconds' %(time.time() - start))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "Now that we have a taste for TensorFlow, lets explore Keras.  What is Keras?  An progamming interface that gives access to various machine learning libraries: TensorFlow / Theano / CNTK (Microsoft Cognitive Toolkit).  Idea: the low-level machine-learning libraries provide access to machine learning algorithms executed on various architectures (e.g. GPU/CPU/TPU (tensor processing unit).  Want one code to be able to access different machine learning libraries to give more confidence for results obtained, benchmark comparisons, leverage best of each library.  \n",
    "\n",
    "There are actually several APIs available, but Keras is the official high-level API of TensorFlow: all the tutorials use Keras to lower the barrier of entry for researchers.\n",
    "- part of core TensorFlow since version 1.4\n",
    "- tensorflow.keras (tf.keras) module\n",
    "- optimized for TF, and better integration with TF-specific features.\n",
    "- contributors to Keras: google, NVIDIA, AWS, Microsoft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras API styles\n",
    "\n",
    "There are three styles to building, training and deploying models with Keras.\n",
    "- sequential model, https://keras.io/models/sequential/.  As the name suggests, this allows single input, single output layer stacks.  It is dead simple, but a reasonable mode for many cases.\n",
    "- functional API: multi-input, multi-output graph topologies.  Kinda like playing with LEGO bricks.  Good for most cases\n",
    "- Model subclassing: most flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through the sequential model approach to solving the above expectancy/birth rate regression model using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# normalize the data\n",
    "mean = data.mean(axis=0)\n",
    "std = data.std(axis=0)\n",
    "\n",
    "norm_data = (data - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the sequential model a densely connected layer, and an output layer that returns a single continuous variable.  The model needs to know what input shape it should expect. This is only needed for the first layer, because the following layers can do automatic shape inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(1,input_dim=1))\n",
    "model.add(layers.Activation('relu')) # relu: rectified linear unit, max(x,0) by default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are a key component of neural networks, in that they introduce non-linear properties to the network.  The main purpose of an activation function is to convert an incoming signal, and transform it (in a non-linear fashion) to an output signal.  There are many different built in activation functions: https://keras.io/activations/.  Some of the more popular include:\n",
    "- relu: rectified linear units.  by default, $f(x)$ returns $\\max{(x,0)}$.  More generally, it supports:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "\\text{max value} & x \\ge \\text{max value} \\\\\n",
    "x & \\text{threshold} \\le x \\le \\text{max value} \\\\\n",
    "\\alpha(x-\\text{threshold}) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "- sigmoid (logistic) \n",
    "\\begin{align}\n",
    "f(x) = \\frac{e^x}{e^x + 1}\n",
    "\\end{align}\n",
    "\n",
    "- hyperbolic tangent\n",
    "\\begin{align}\n",
    "f(x) = \\tanh(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to configure the model for training.  This is accomplished by calling the compile function.  https://keras.io/models/sequential/.  We need to specify the optimizer, the loss function and an approriate list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', # scaled gradient descent\n",
    "              loss='mse', # mean squared error\n",
    "              metrics=['mae']) # mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(norm_data[:,0], norm_data[:,1],\n",
    "          epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see how the fit went.  Normally, we would split the data into a training set and a test set, but I didn't do that here so we can compare to the regression fit above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_x = np.linspace(1,7,100)\n",
    "regress_x = (regress_x - mean[0])/std[0]\n",
    "regress_y = model.predict(regress_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(norm_data[:,0], norm_data[:,1],marker='.')\n",
    "plt.plot(regress_x,regress_y,color='blue',linewidth=3)\n",
    "plt.xlabel('normalized birth rate')\n",
    "_ = plt.ylabel('normalized life expectency (years)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, there's something I'm mis-understanding about the fitting (or maybe the prediction?) ... looks non-linear.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets repeat again, but without the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(1,input_dim=1))\n",
    "\n",
    "model.compile(optimizer='rmsprop', # scaled gradient descent\n",
    "              loss='mse', # mean squared error\n",
    "              metrics=['mae']) # mean absolute error\n",
    "\n",
    "model.fit(norm_data[:,0], norm_data[:,1],\n",
    "          epochs=10,\n",
    "          validation_split=0.2)\n",
    "\n",
    "regress_x = np.linspace(1,7,100)\n",
    "regress_x = (regress_x - mean[0])/std[0]\n",
    "regress_y = model.predict(regress_x)\n",
    "\n",
    "plt.scatter(norm_data[:,0], norm_data[:,1],marker='.')\n",
    "plt.plot(regress_x,regress_y,color='blue',linewidth=3)\n",
    "plt.xlabel('normalized birth rate')\n",
    "_ = plt.ylabel('normalized life expectency (years)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm, certainly appears to be the non-linear activation function that gave the earlier non-linear behavior.  Probably with a larger data set where TensorFlow can do a better job with cross validation, this behavior is seldom noticed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
